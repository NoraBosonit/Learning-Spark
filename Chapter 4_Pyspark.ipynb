{"cells":[{"cell_type":"markdown","source":["# Chapter 4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e9021fd-372d-4ffc-8d39-05d084062c60"}}},{"cell_type":"markdown","source":["## Creamos una vista temporal"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75839af4-251c-4c72-a765-7aa41a70f3c9"}}},{"cell_type":"code","source":["schema = \"`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"751d1d90-a031-435f-ad6c-0687494cd17d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# In Python\nfrom pyspark.sql import SparkSession \n# Create a SparkSession\nspark = (SparkSession\n .builder\n .appName(\"SparkSQLExampleApp\")\n .getOrCreate())\n# Path to data set\ncsv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n# Read and create a temporary view\n# Infer schema (note that for larger files you \n# may want to specify the schema)\ndf = (spark.read.schema(schema).format(\"csv\")\n .option(\"header\", \"true\")\n .load(csv_file))\ndf.createOrReplaceTempView(\"us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec7e5be9-57fe-431a-a17e-f921fde90b1b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d13e8bd9-fa85-4e28-a89f-1bad22388673"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: ['date', 'delay', 'distance', 'origin', 'destination']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: ['date', 'delay', 'distance', 'origin', 'destination']"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6702fa03-a58c-41ba-8dcf-1cb745a7d746"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n|01031245|   -4|     602|   ABE|        ATL|\n+--------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n|01031245|   -4|     602|   ABE|        ATL|\n+--------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Ejercicio: Convertir la columna fecha a un formato legible**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7519cd24-9225-47cd-b1a6-2975d22c912b"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf_new = (df\n         .withColumn(\"DateAndHour\", to_timestamp(col(\"date\"), \"mm-dd hh:mm\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73cce271-a983-4613-969c-c7b8fc2ad212"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_new.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b93d67fc-b8aa-410c-a979-10f900fb114c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+-----------+\n|    date|delay|distance|origin|destination|DateAndHour|\n+--------+-----+--------+------+-----------+-----------+\n|01011245|    6|     602|   ABE|        ATL|       null|\n|01020600|   -8|     369|   ABE|        DTW|       null|\n|01021245|   -2|     602|   ABE|        ATL|       null|\n|01020605|   -4|     602|   ABE|        ATL|       null|\n|01031245|   -4|     602|   ABE|        ATL|       null|\n|01030605|    0|     602|   ABE|        ATL|       null|\n|01041243|   10|     602|   ABE|        ATL|       null|\n|01040605|   28|     602|   ABE|        ATL|       null|\n|01051245|   88|     602|   ABE|        ATL|       null|\n|01050605|    9|     602|   ABE|        ATL|       null|\n|01061215|   -6|     602|   ABE|        ATL|       null|\n|01061725|   69|     602|   ABE|        ATL|       null|\n|01061230|    0|     369|   ABE|        DTW|       null|\n|01060625|   -3|     602|   ABE|        ATL|       null|\n|01070600|    0|     369|   ABE|        DTW|       null|\n|01071725|    0|     602|   ABE|        ATL|       null|\n|01071230|    0|     369|   ABE|        DTW|       null|\n|01070625|    0|     602|   ABE|        ATL|       null|\n|01071219|    0|     569|   ABE|        ORD|       null|\n|01080600|    0|     369|   ABE|        DTW|       null|\n+--------+-----+--------+------+-----------+-----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+-----------+\n|    date|delay|distance|origin|destination|DateAndHour|\n+--------+-----+--------+------+-----------+-----------+\n|01011245|    6|     602|   ABE|        ATL|       null|\n|01020600|   -8|     369|   ABE|        DTW|       null|\n|01021245|   -2|     602|   ABE|        ATL|       null|\n|01020605|   -4|     602|   ABE|        ATL|       null|\n|01031245|   -4|     602|   ABE|        ATL|       null|\n|01030605|    0|     602|   ABE|        ATL|       null|\n|01041243|   10|     602|   ABE|        ATL|       null|\n|01040605|   28|     602|   ABE|        ATL|       null|\n|01051245|   88|     602|   ABE|        ATL|       null|\n|01050605|    9|     602|   ABE|        ATL|       null|\n|01061215|   -6|     602|   ABE|        ATL|       null|\n|01061725|   69|     602|   ABE|        ATL|       null|\n|01061230|    0|     369|   ABE|        DTW|       null|\n|01060625|   -3|     602|   ABE|        ATL|       null|\n|01070600|    0|     369|   ABE|        DTW|       null|\n|01071725|    0|     602|   ABE|        ATL|       null|\n|01071230|    0|     369|   ABE|        DTW|       null|\n|01070625|    0|     602|   ABE|        ATL|       null|\n|01071219|    0|     569|   ABE|        ORD|       null|\n|01080600|    0|     369|   ABE|        DTW|       null|\n+--------+-----+--------+------+-----------+-----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Consultas SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7b26e20-5026-41b0-9021-fadc2d8f205b"}}},{"cell_type":"markdown","source":["Buscamos todos los vuelos cuya distancia sea mayor a 1,000 millas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d629db21-ca5b-4c7b-8e51-bdfb1ab1ffbd"}}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT distance, origin, destination \nFROM us_delay_flights_tbl WHERE distance > 1000 \nORDER BY distance DESC\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a27fb4f-0c6e-4759-bc02-c85cf1fe653c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Todos los vuelos entre San Francisco (SFO) y Chicago (ORD) con al menos dos horas de retraso"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50dbe4be-8ef0-4a67-8426-27642a74a540"}}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT date, delay, origin, destination \nFROM us_delay_flights_tbl \nWHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \nORDER by delay DESC\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00f257d8-81d7-4231-b71b-0c556bc74b7e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|02190925| 1638|   SFO|        ORD|\n|01031755|  396|   SFO|        ORD|\n|01022330|  326|   SFO|        ORD|\n|01051205|  320|   SFO|        ORD|\n|01190925|  297|   SFO|        ORD|\n|02171115|  296|   SFO|        ORD|\n|01071040|  279|   SFO|        ORD|\n|01051550|  274|   SFO|        ORD|\n|03120730|  266|   SFO|        ORD|\n|01261104|  258|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|02190925| 1638|   SFO|        ORD|\n|01031755|  396|   SFO|        ORD|\n|01022330|  326|   SFO|        ORD|\n|01051205|  320|   SFO|        ORD|\n|01190925|  297|   SFO|        ORD|\n|02171115|  296|   SFO|        ORD|\n|01071040|  279|   SFO|        ORD|\n|01051550|  274|   SFO|        ORD|\n|03120730|  266|   SFO|        ORD|\n|01261104|  258|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Etiquetar todos los vuelos de EE. UU., independientemente de su origen y destino, con una indicación de los retrasos que experimentaron: retrasos muy largos (> 6 horas), retrasos largos (2 a 6 horas), etc. Agregaremos estas etiquetas legibles por humanos en una nueva columna llamada Flight_Delays"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8285acd-088d-48b7-94f6-c2338a4761f9"}}},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT delay, origin, destination,\n CASE \n WHEN delay > 360 THEN 'Very Long Delays'\n WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n WHEN delay = 0 THEN 'No Delays'\n ELSE 'Early'\n END AS Flight_Delays\n FROM us_delay_flights_tbl\n ORDER BY origin, delay DESC\"\"\").show(10)\n#Supongo que CASE será para crear nuevas columnas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad2ce0c8-1429-4548-85f1-2ccf1f17a0f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+-----------+-------------+\n|delay|origin|destination|Flight_Delays|\n+-----+------+-----------+-------------+\n|  333|   ABE|        ATL|  Long Delays|\n|  305|   ABE|        ATL|  Long Delays|\n|  275|   ABE|        ATL|  Long Delays|\n|  257|   ABE|        ATL|  Long Delays|\n|  247|   ABE|        DTW|  Long Delays|\n|  247|   ABE|        ATL|  Long Delays|\n|  219|   ABE|        ORD|  Long Delays|\n|  211|   ABE|        ATL|  Long Delays|\n|  197|   ABE|        DTW|  Long Delays|\n|  192|   ABE|        ORD|  Long Delays|\n+-----+------+-----------+-------------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+-----------+-------------+\n|delay|origin|destination|Flight_Delays|\n+-----+------+-----------+-------------+\n|  333|   ABE|        ATL|  Long Delays|\n|  305|   ABE|        ATL|  Long Delays|\n|  275|   ABE|        ATL|  Long Delays|\n|  257|   ABE|        ATL|  Long Delays|\n|  247|   ABE|        DTW|  Long Delays|\n|  247|   ABE|        ATL|  Long Delays|\n|  219|   ABE|        ORD|  Long Delays|\n|  211|   ABE|        ATL|  Long Delays|\n|  197|   ABE|        DTW|  Long Delays|\n|  192|   ABE|        ORD|  Long Delays|\n+-----+------+-----------+-------------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Las consultas anteriores se pueden expresar como consultas API. Por ejemplo la primera:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55b446aa-ee22-4b8d-b6e4-8091aa6c7fb7"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, desc\n(df.select(\"distance\", \"origin\", \"destination\")\n .where(col(\"distance\") > 1000)\n .orderBy(desc(\"distance\"))).show(10)\n# Or\n(df.select(\"distance\", \"origin\", \"destination\")\n .where(\"distance > 1000\")\n .orderBy(\"distance\", ascending=False).show(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57ba6f7c-d34a-4ba0-8238-fd6fa54684ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Ejercicio: Hacer las otras 2 consultas usando la API de DataFrame**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0317ac2-e24a-468d-8eaa-358b5ad030af"}}},{"cell_type":"markdown","source":["La segunda"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60b8a1c4-bb35-4f4e-ab60-368c905d0c60"}}},{"cell_type":"code","source":["#spark.sql(\"\"\"SELECT date, delay, origin, destination \n#FROM us_delay_flights_tbl \n#WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n#ORDER by delay DESC\"\"\").show(10)\n\n(df\n .select(df.date, df.delay, df.origin, df.destination)\n .where((df.delay > 120) & (df.origin == 'SFO') & (df.destination == 'ORD'))\n .orderBy(df.delay, ascending = False)\n .show(5)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba468439-66ff-45eb-99b1-d21bfbf7d514"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|02190925| 1638|   SFO|        ORD|\n|01031755|  396|   SFO|        ORD|\n|01022330|  326|   SFO|        ORD|\n|01051205|  320|   SFO|        ORD|\n|01190925|  297|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|02190925| 1638|   SFO|        ORD|\n|01031755|  396|   SFO|        ORD|\n|01022330|  326|   SFO|        ORD|\n|01051205|  320|   SFO|        ORD|\n|01190925|  297|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["La tercera"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"383e9918-4731-4662-883f-505add033dad"}}},{"cell_type":"code","source":["#spark.sql(\"\"\"SELECT delay, origin, destination,\n#CASE\n#WHEN delay > 360 THEN 'Very Long Delays'\n#WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n#WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n#WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n#WHEN delay = 0 THEN 'No Delays'\n#ELSE 'Early'\n#END AS Flight_Delays\n#FROM us_delay_flights_tbl\n#ORDER BY origin, delay DESC\"\"\").show(10)\nfrom pyspark.sql.functions import *\ndf_3 = (df.withColumn(\"Flight_Delays\", \\\n                        when((df.delay > 360), lit(\"Very Long Delays\")) \\\n                        when((df.delay > 120) & (df.delay < 360), lit(\"Long Delays\")) \\\n                        when(col(\"delay\") > 60 & col(\"delay\") < 120,\n                            lit(\"Short Delays\"))\n                        when(col(\"delay\") > 0 & col(\"delay\") < 60,\n                            lit(\"Tolerable Delays\"))\n                        when(col(\"delay\") == 0,\n                            lit(\"No Delays\"))\n                        otherwise(lit(\"Early\")))\n.orderBy(df.origin, ascending=False)\n.show(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee6798d1-2ea0-42e3-b613-0c8b8106b617"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-447741929212287>\"\u001B[0;36m, line \u001B[0;32m15\u001B[0m\n\u001B[0;31m    when((df.delay > 120) & (df.delay < 360), lit(\"Long Delays\")) \\\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-447741929212287>, line 15)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-447741929212287>\"\u001B[0;36m, line \u001B[0;32m15\u001B[0m\n\u001B[0;31m    when((df.delay > 120) & (df.delay < 360), lit(\"Long Delays\")) \\\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf_3 = df.withColumn(\"Flight_Delays\", \\\n                     when((df.delay > 360), lit(\"Very Long Delays\")) \\\n                     when((df.delay < 120), lit(\"C\"))\n                    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"659c299f-e75c-46c5-b17c-3903d05917bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-447741929212288>\"\u001B[0;36m, line \u001B[0;32m4\u001B[0m\n\u001B[0;31m    when((df.delay < 120), lit(\"C\"))\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-447741929212288>, line 4)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-447741929212288>\"\u001B[0;36m, line \u001B[0;32m4\u001B[0m\n\u001B[0;31m    when((df.delay < 120), lit(\"C\"))\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_3.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58498a09-0be1-4fc7-b35e-3a882ecedd78"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+-------------+\n|    date|delay|distance|origin|destination|Flight_Delays|\n+--------+-----+--------+------+-----------+-------------+\n|01011245|    6|     602|   ABE|        ATL|         null|\n|01020600|   -8|     369|   ABE|        DTW|         null|\n|01021245|   -2|     602|   ABE|        ATL|         null|\n|01020605|   -4|     602|   ABE|        ATL|         null|\n|01031245|   -4|     602|   ABE|        ATL|         null|\n+--------+-----+--------+------+-----------+-------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+-------------+\n|    date|delay|distance|origin|destination|Flight_Delays|\n+--------+-----+--------+------+-----------+-------------+\n|01011245|    6|     602|   ABE|        ATL|         null|\n|01020600|   -8|     369|   ABE|        DTW|         null|\n|01021245|   -2|     602|   ABE|        ATL|         null|\n|01020605|   -4|     602|   ABE|        ATL|         null|\n|01031245|   -4|     602|   ABE|        ATL|         null|\n+--------+-----+--------+------+-----------+-------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Creando Tablas y Databases de SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04d2d2fe-3861-4e31-9186-0289f344812a"}}},{"cell_type":"markdown","source":["### Databases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e995b5e5-fd2b-464b-98e7-44898f60cbf8"}}},{"cell_type":"code","source":["spark.sql(\"CREATE DATABASE learn_spark_db\")\nspark.sql(\"USE learn_spark_db\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba19e301-6f9a-4ab1-8567-8ec7581a1e76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[45]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[45]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ahora todas las tablas que se creen estarán dentro de esa base de datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"130a9715-6fe8-4c9a-a12d-4f28e72c2b91"}}},{"cell_type":"markdown","source":["### Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"761f6ddf-701d-4b7d-b4cb-ffe4b333871a"}}},{"cell_type":"markdown","source":["#### Managed\nLas tablas administradas administran tanto los metadatos como los datos. Si se ejecuta un DROP se eliminarían los datos reales y los metadatos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ddaa9b5-1804-4761-9352-952e77289fff"}}},{"cell_type":"code","source":["spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ab21799-2e3d-459d-baf0-ae6eb57d4a52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[72]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[72]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DROP table managed_us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cee51fa4-ff74-4579-902d-a7f512269a47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[74]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[74]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Utilizando la API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74bcac9d-0c0d-4ddf-8df1-6ce61a22149a"}}},{"cell_type":"code","source":["csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n# Schema as defined in the preceding example\nschema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\nflights_df = spark.read.csv(csv_file, schema=schema)\nflights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"615a30e4-128c-48a4-843a-5e7d1b3328c1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["flights_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ff6c57f-0c6f-47e0-9be2-8fa9ffe30ed9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|    date| null|    null|origin|destination|\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n|01031245|   -4|     602|   ABE|        ATL|\n|01030605|    0|     602|   ABE|        ATL|\n|01041243|   10|     602|   ABE|        ATL|\n|01040605|   28|     602|   ABE|        ATL|\n|01051245|   88|     602|   ABE|        ATL|\n|01050605|    9|     602|   ABE|        ATL|\n|01061215|   -6|     602|   ABE|        ATL|\n|01061725|   69|     602|   ABE|        ATL|\n|01061230|    0|     369|   ABE|        DTW|\n|01060625|   -3|     602|   ABE|        ATL|\n|01070600|    0|     369|   ABE|        DTW|\n|01071725|    0|     602|   ABE|        ATL|\n|01071230|    0|     369|   ABE|        DTW|\n|01070625|    0|     602|   ABE|        ATL|\n|01071219|    0|     569|   ABE|        ORD|\n+--------+-----+--------+------+-----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|    date| null|    null|origin|destination|\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n|01031245|   -4|     602|   ABE|        ATL|\n|01030605|    0|     602|   ABE|        ATL|\n|01041243|   10|     602|   ABE|        ATL|\n|01040605|   28|     602|   ABE|        ATL|\n|01051245|   88|     602|   ABE|        ATL|\n|01050605|    9|     602|   ABE|        ATL|\n|01061215|   -6|     602|   ABE|        ATL|\n|01061725|   69|     602|   ABE|        ATL|\n|01061230|    0|     369|   ABE|        DTW|\n|01060625|   -3|     602|   ABE|        ATL|\n|01070600|    0|     369|   ABE|        DTW|\n|01071725|    0|     602|   ABE|        ATL|\n|01071230|    0|     369|   ABE|        DTW|\n|01070625|    0|     602|   ABE|        ATL|\n|01071219|    0|     569|   ABE|        ORD|\n+--------+-----+--------+------+-----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Aunque cierre un cluster y abra otro, lo que he hecho en sesiones anteriores se guarda en el fichero de metadatos. Al abrir otra sesión, la tabla no aparece en el contenido de las tablas, pero no me deja volverla a crear porque ya existen los metadatos de la misma. Tendré que eliminar el registro"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d3c8aca-2c77-42f1-ac4d-e779e07f4823"}}},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/user/hive/warehouse/learn_spark_db.db/managed_us_delay_flights_tbl\",recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d132bd21-f38d-48ed-b3bf-9286e0e7de6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[66]: False","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[66]: False"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DROP table managed_us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eafdc0ef-d65f-4299-97e9-91b60fe003cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[79]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[79]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Unmanaged\nEn las tablas no administradas, Spark solo administra los metadatos mientras que el propio usuario maneja los datos. Aquí el DROP solo eliminaría los metadatos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24ce703d-c278-41be-9cbd-af00e06045d7"}}},{"cell_type":"code","source":["#SQL\nspark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n distance INT, origin STRING, destination STRING)\n USING csv OPTIONS (PATH\n '/learning-spark-v2/flights/departuredelays.csv')\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb533f4a-bb21-45de-8832-1e6b32ad8dab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[149]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[149]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Para eliminar se ha de especificar la ruta de los datos ya que en las unmanaged tables, Spark solo controla los metadatos y no sabe dónde se especifican los datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d63456f6-697e-46ac-8109-8f2fb5f57eff"}}},{"cell_type":"code","source":["spark.sql(\"\"\"DROP TABLE IF EXISTS us_delay_flights_tbl\"\"\") #deletes the metadata\ndbutils.fs.rm(\"/learning-spark-v2/flights/departuredelays.csv\", True)   # deletes the data\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd7a7902-4c6e-44bf-ab10-c81f8bcc1232"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[148]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[148]: True"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nls 'dbfs:/databricks-datasets/'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12f2eea7-a3f8-4483-8af2-af7488b722f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/","databricks-datasets/",0,0],["dbfs:/databricks-datasets/COVID/","COVID/",0,0],["dbfs:/databricks-datasets/README.md","README.md",976,1532468253000],["dbfs:/databricks-datasets/Rdatasets/","Rdatasets/",0,0],["dbfs:/databricks-datasets/SPARK_README.md","SPARK_README.md",3359,1455043490000],["dbfs:/databricks-datasets/adult/","adult/",0,0],["dbfs:/databricks-datasets/airlines/","airlines/",0,0],["dbfs:/databricks-datasets/amazon/","amazon/",0,0],["dbfs:/databricks-datasets/asa/","asa/",0,0],["dbfs:/databricks-datasets/atlas_higgs/","atlas_higgs/",0,0],["dbfs:/databricks-datasets/bikeSharing/","bikeSharing/",0,0],["dbfs:/databricks-datasets/cctvVideos/","cctvVideos/",0,0],["dbfs:/databricks-datasets/credit-card-fraud/","credit-card-fraud/",0,0],["dbfs:/databricks-datasets/cs100/","cs100/",0,0],["dbfs:/databricks-datasets/cs110x/","cs110x/",0,0],["dbfs:/databricks-datasets/cs190/","cs190/",0,0],["dbfs:/databricks-datasets/data.gov/","data.gov/",0,0],["dbfs:/databricks-datasets/definitive-guide/","definitive-guide/",0,0],["dbfs:/databricks-datasets/delta-sharing/","delta-sharing/",0,0],["dbfs:/databricks-datasets/flights/","flights/",0,0],["dbfs:/databricks-datasets/flower_photos/","flower_photos/",0,0],["dbfs:/databricks-datasets/flowers/","flowers/",0,0],["dbfs:/databricks-datasets/genomics/","genomics/",0,0],["dbfs:/databricks-datasets/hail/","hail/",0,0],["dbfs:/databricks-datasets/identifying-campaign-effectiveness/","identifying-campaign-effectiveness/",0,0],["dbfs:/databricks-datasets/iot/","iot/",0,0],["dbfs:/databricks-datasets/iot-stream/","iot-stream/",0,0],["dbfs:/databricks-datasets/learning-spark/","learning-spark/",0,0],["dbfs:/databricks-datasets/learning-spark-v2/","learning-spark-v2/",0,0],["dbfs:/databricks-datasets/lending-club-loan-stats/","lending-club-loan-stats/",0,0],["dbfs:/databricks-datasets/med-images/","med-images/",0,0],["dbfs:/databricks-datasets/media/","media/",0,0],["dbfs:/databricks-datasets/mnist-digits/","mnist-digits/",0,0],["dbfs:/databricks-datasets/news20.binary/","news20.binary/",0,0],["dbfs:/databricks-datasets/nyctaxi/","nyctaxi/",0,0],["dbfs:/databricks-datasets/nyctaxi-with-zipcodes/","nyctaxi-with-zipcodes/",0,0],["dbfs:/databricks-datasets/online_retail/","online_retail/",0,0],["dbfs:/databricks-datasets/overlap-join/","overlap-join/",0,0],["dbfs:/databricks-datasets/power-plant/","power-plant/",0,0],["dbfs:/databricks-datasets/retail-org/","retail-org/",0,0],["dbfs:/databricks-datasets/rwe/","rwe/",0,0],["dbfs:/databricks-datasets/sai-summit-2019-sf/","sai-summit-2019-sf/",0,0],["dbfs:/databricks-datasets/sample_logs/","sample_logs/",0,0],["dbfs:/databricks-datasets/samples/","samples/",0,0],["dbfs:/databricks-datasets/sfo_customer_survey/","sfo_customer_survey/",0,0],["dbfs:/databricks-datasets/sms_spam_collection/","sms_spam_collection/",0,0],["dbfs:/databricks-datasets/songs/","songs/",0,0],["dbfs:/databricks-datasets/structured-streaming/","structured-streaming/",0,0],["dbfs:/databricks-datasets/timeseries/","timeseries/",0,0],["dbfs:/databricks-datasets/tpch/","tpch/",0,0],["dbfs:/databricks-datasets/warmup/","warmup/",0,0],["dbfs:/databricks-datasets/weather/","weather/",0,0],["dbfs:/databricks-datasets/wiki/","wiki/",0,0],["dbfs:/databricks-datasets/wikipedia-datasets/","wikipedia-datasets/",0,0],["dbfs:/databricks-datasets/wine-quality/","wine-quality/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#API\n(flights_df\n .write\n .option(\"path\", \"/learning-spark-v2/flights/departuredelays.csv\")\n .saveAsTable(\"us_delay_flights_tbl\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f39bfed-3407-4fc4-8100-73625ef9ec29"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Creando vistas\nLas vistas se crean sobre tablas existentes y pueden ser globales o session-scoped. Las globales son visibles en todas las Spark-Sessions de un cluster y la segunda solo es visible para una sola Spark-session (son temporales, desaparecen después de la finalización de la Spark Application."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b4ee1eb-abaf-4230-81c8-21ac463b7275"}}},{"cell_type":"markdown","source":["#### Global\nSon visibles en todos los SparkSession en un cluster dado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2180cca-fc31-4d72-b752-f016bbb29a32"}}},{"cell_type":"code","source":["#A partir de una tabla existente: SQL\nspark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n origin = 'SFO'\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dd0819a-e199-403e-bc96-73f483b3738b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[89]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[89]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["#API\ndf_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM \n us_delay_flights_tbl WHERE origin = 'SFO'\"\"\")\ndf_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\ndf_sfo.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d78bc93-96a6-428a-9d24-26a8cc7bb964"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|01011250|   55|   SFO|        JFK|\n|01012230|    0|   SFO|        JFK|\n|01010705|   -7|   SFO|        JFK|\n|01010620|   -3|   SFO|        MIA|\n|01010915|   -3|   SFO|        LAX|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|01011250|   55|   SFO|        JFK|\n|01012230|    0|   SFO|        JFK|\n|01010705|   -7|   SFO|        JFK|\n|01010620|   -3|   SFO|        MIA|\n|01010915|   -3|   SFO|        LAX|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Eliminar vistas**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e975bde-0c0a-4919-9dd8-c7771cc39069"}}},{"cell_type":"code","source":["spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6b22ff6-c8b2-49b4-a865-248e0de30495"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Temporal\nLas vistas son visibles para un único SparkSessios y se eliminan después de finalizar la aplicación Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff1ab11d-a5be-4f27-8f80-7f6e30676eb2"}}},{"cell_type":"code","source":["spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n origin = 'JFK'\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c506731-77fc-4aab-b4f8-acbeb68e8649"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[95]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[95]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["#API\ndf_jfk = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM \n us_delay_flights_tbl WHERE origin = 'JFK'\"\"\")\ndf_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\ndf_jfk.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d36e84f4-2d23-41f9-8680-6aa10303dc60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|01010900|   14|   JFK|        LAX|\n|01011200|   -3|   JFK|        LAX|\n|01011900|    2|   JFK|        LAX|\n|01011700|   11|   JFK|        LAS|\n|01010800|   -1|   JFK|        SFO|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|01010900|   14|   JFK|        LAX|\n|01011200|   -3|   JFK|        LAX|\n|01011900|    2|   JFK|        LAX|\n|01011700|   11|   JFK|        LAS|\n|01010800|   -1|   JFK|        SFO|\n+--------+-----+------+-----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Eliminar vistas**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e751f903-96fb-4bb1-9e40-55393b60267d"}}},{"cell_type":"code","source":["spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2809a5f9-df6d-4430-ae81-4dcae377a71f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Metadatos\nSpark administra los metadatos asociados a cada tabla, ya sea administrada o no administrada. Para administrar los metadatos se utiliza Catalog la cual es una herramienta de alto nivel de Spark SQL para almacenar metadatos.\n\nDespués de crear la vartiable de la SparkSession, se puede acceder al metadata almacenado de la siaguiente manera:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1984b5b3-2287-40ba-b383-506635bbc34c"}}},{"cell_type":"code","source":["#spark.catalog.listDatabases()\n#spark.catalog.listTables()\nspark.catalog.listColumns(\"us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c173856-936b-4167-b7c3-4b5604e564de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[109]: [Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[109]: [Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Almacenamiento en caché de tablas SQL**\n\nSe puede cache y uncahe tablas SQL y vistas. Además, si especificas la tabla como LAZY, se guardará en caché cuando se utiliza la tabla por primera vez y no inmediatamente cuando se crea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"280f30e5-161a-4847-a2d0-54a5ad2b7771"}}},{"cell_type":"code","source":["#In SQL\nspark.sql(\"CACHE [LAZY] TABLE <table-name>\")\nspark.sql(\"UNCACHE TABLE <table-name>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0255abf5-c645-416d-a3cc-340e1e2a5bb4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Leyendo tablas a DataFrames\nSi ya hay una base de datos learn_spark_db y una tabla us_delay_flights_tbl preparadas para ser utilizadas. En vez de importar datos directamente del JSON file para tener un dataframe, podemos simplemente ejecutar una consulta SQL a la tabla y asignarle como resultado un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26a58071-e2e5-4f3c-9fe3-cefcf9521029"}}},{"cell_type":"code","source":["#Dos formas\nusFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\nusFlightsDF2 = spark.table(\"us_delay_flights_tbl\")\nusFlightsDF.show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae797229-ad21-465a-b831-2fe2f34dc534"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|    date| null|    null|origin|destination|\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n+--------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-----+--------+------+-----------+\n|    date|delay|distance|origin|destination|\n+--------+-----+--------+------+-----------+\n|    date| null|    null|origin|destination|\n|01011245|    6|     602|   ABE|        ATL|\n|01020600|   -8|     369|   ABE|        DTW|\n|01021245|   -2|     602|   ABE|        ATL|\n|01020605|   -4|     602|   ABE|        ATL|\n+--------+-----+--------+------+-----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data Sources for DataFrames and SQL Tables\nSpark SQL proporciona una gran variedad de data sources. Además de proporcionar un conjunto de métodos reales para leer y escribir desde estas fuentes utilizando la Data Sources API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"186d3f6e-6a58-4c88-b9e5-c63f678eac15"}}},{"cell_type":"markdown","source":["### DataFrameReader\nEs la herramienta con la que se lee una fuente de datos en un DataFrame. Tiene un formato definido y un patrón recomendado de uso.\n\n```DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23ce616e-a116-46ef-98c1-f1f8431c4421"}}},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d855bed1-621a-46b9-9847-b00b3113fa79"}}},{"cell_type":"markdown","source":["| Method | Arguments | Description |\n| --- | --- | --- |\n| format() | \"parquet\", \"csv\", \"txt\", \"json\",\"jdbc\", \"orc\", \"avro\", etc. | If you don’t specify this method, then the default ismParquet or whatever is set in spark.sql.sources.default.|\n| option() |(\"mode\", {PERMISSIVE / FAILFAST / DROPMALFORMED } )(\"inferSchema\", {true / false}) (\"path\", \"path_file_data_source\") | A series of key/value pairs and options. The Spark documentation shows some examples and explains the different modes and their actions. The default mode is PERMISSIVE. The \"inferSchema\" and \"mode\" options are specific to the JSON and CSV file formats.|\n| schema() | DDL String or StructType, e.g., 'A INT, B STRING' orStructType(...) | For JSON or CSV format, you can specify to infer the schema in the option() method. Generally, providing a schema for any format makes loading faster and ensures your data conforms to the expected schema.|\n| load() | \"/path/to/data/source\" | The path to the data source. This can be empty if specified in option(\"path\", \"...\").|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24ef5042-03ac-4e75-b1ea-18a0d5524072"}}},{"cell_type":"markdown","source":["**Ejemplos**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a07e142f-4049-4c75-a9be-e8aa65989316"}}},{"cell_type":"code","source":["#Use Parquet\nfile = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\"\"\ndf = spark.read.format(\"parquet\").load(file)\n#df2 = spark.read.load(file)\n#Use CSV\ndf3 = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\").load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")\n#Use JSON\ndf4 = spark.read.format(\"json\").load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\n\ndf.show(5)\ndf3.show(5)\ndf4.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d741023c-209a-46e4-b6c6-784bacbaa2a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["En general, no se necesita ningún esquema cuando se lee desde una fuente de datos estática de Parquet; los metadatos de Parquet generalmente contienen el esquema, por lo que se deduce."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125a5e9e-6233-4639-9a1a-36b8f90cfe41"}}},{"cell_type":"markdown","source":["### DataFrameWriter\nGuarda o escribe datos en una fuente de datos integrada especificada y estos son sus patrones de uso:\n```\nDataFrameWriter.format(args)\n .option(args)\n .bucketBy(args)\n .partitionBy(args)\n .save(path)\n```\no\n\n```\nDataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"323166bd-2576-4925-8056-1d0c94f8b9ae"}}},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ddbdd9b-f453-41f7-a8dd-7fbebcc94924"}}},{"cell_type":"markdown","source":["| Method | Arguments | Description |\n| --- | --- | --- |\n| format() | \"parquet\", \"csv\", \"txt\", \"json\",\"jdbc\", \"orc\", \"avro\", etc. | If you don’t specify this method, then the default is Parquet or whatever is set in spark.sql.sources.default.|\n| option() | (\"mode\", {append / overwrite / ignore / error or errorifexists} ) (\"mode\", {SaveMode.Overwrite / SaveMode.Append, Save Mode.Ignore, SaveMode.ErrorIfExists}) (\"path\", \"path_to_write_to\") | A series of key/value pairs and options. The Spark documentation shows some examples. This is an overloaded method. The default mode options are error or error ifexists and SaveMode.ErrorIfExists; they throw an exception at runtime if the data already exists.|\n| bucketBy() | (numBuckets, col, col..., coln) | The number of buckets and names of columns to bucket by. Uses Hive’s bucketing scheme on a filesystem.|\n| save() | \"/path/to/data/source\" | The path to save to. This can be empty if specified in option(\"path\", \"...\"). |\n| saveAsTable() | \"table_name\" | The table to save to.|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69c35a55-f8d7-4564-b478-b7c3e79cc93c"}}},{"cell_type":"markdown","source":["**Ejemplo**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd83458-7f2d-4687-ac88-367b0730e85a"}}},{"cell_type":"code","source":["#Use JSON\nlocation = \"\"\"\"/databricks-datasets/learning-spark-v2/\n sf-fire/Ejemplo_save_json.json\"\"\"\ndf.write.format(\"json\").mode(\"overwrite\").save(location)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"601a28ae-d487-4ae3-bf79-c8844539d0d8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nls /learning-spark-v2/sf-fire/Ejemplo_save_json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e8fbd73-9774-4969-be06-e78406fc5d0c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS","_SUCCESS",0,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276","_committed_2238006556770501276",114,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276","_started_2238006556770501276",0,1651142576000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json","part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json",21353,1651142576000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276</td><td>_committed_2238006556770501276</td><td>114</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276</td><td>_started_2238006556770501276</td><td>0</td><td>1651142576000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>21353</td><td>1651142576000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Fuentes de datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a64cd1d0-b9f7-4072-ba97-d5f732835044"}}},{"cell_type":"markdown","source":["### Parquet\nLa fuente de datos predeterminada de Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed33013a-72be-4ddf-88c1-3e84dcfc956e"}}},{"cell_type":"markdown","source":["#### Leer de archivos Parquet en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f94de3ea-54bd-412b-a937-a9a29353d044"}}},{"cell_type":"code","source":["file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\ndf = spark.read.format(\"parquet\").load(file)\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34f98267-d472-4edd-8025-45668e7cc117"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos Parquet en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae863ae4-c910-4d68-b637-08e48d966468"}}},{"cell_type":"code","source":["#Creamos la tabla\nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING parquet\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"914994e0-9ddf-4421-b564-99595961ca27"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[154]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[154]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["#Leemos la tabla\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c675b70-e03d-4abe-8a3b-e05caa9a4bcd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos Parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34a65369-9e07-43ce-a013-cab93376a480"}}},{"cell_type":"code","source":["(df.write.format(\"parquet\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/parquet/df_parquet\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4565f060-d05b-461f-b22a-af52d1334339"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DROP table us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37e88a16-fc0f-4936-8e23-0c1abed0d75d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[162]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[162]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a tablas SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1d9486c-d27b-4dca-b4d5-aadc27205377"}}},{"cell_type":"code","source":["(df.write\n .mode(\"overwrite\")\n .saveAsTable(\"us_delay_flights_tbl\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b77dca9f-a3c0-45d6-b5f3-33749d1ef165"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### JSON\nEn Spark se soporta tanto el formato sigle-line mode como el multiline mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c758f79-7e15-4df1-95e9-8652bf3f550d"}}},{"cell_type":"markdown","source":["#### Leer de archivos JSON en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be382380-3003-4961-8638-0fb4456bca9d"}}},{"cell_type":"code","source":["file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\ndf = spark.read.format(\"json\").load(file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a33ae7c3-f7a3-4f3a-8543-62a02d11bd3b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos JSON en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04944234-832a-4888-be18-6bb166c104a1"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING json\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acaf9f63-23be-4c56-b834-f8f8b15cb17c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03eccda5-9311-4e94-8990-20d14840e06d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos JSON"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38701210-4740-4bfc-b93a-35578faee91f"}}},{"cell_type":"code","source":["(df.write.format(\"json\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/json/df_json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5527aa73-e7bc-4308-abfe-a305db8211c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-447741929212383>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m (df.write.format(\"json\")\n\u001B[0m\u001B[1;32m      2\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"snappy\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m  .save(\"/tmp/data/json/df_json\"))\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    738\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    739\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 740\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    741\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    742\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3738.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:603)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:359)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:209)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:356)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:306)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:183)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:427)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 204.0 failed 1 times, most recent failure: Lost task 2.0 in stage 204.0 (TID 355) (ip-10-172-200-232.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:607)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:456)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I\n\tat org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)\n\tat org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.compress(BlockCompressorStream.java:149)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.finish(BlockCompressorStream.java:142)\n\tat org.apache.hadoop.io.compress.CompressionOutputStream.close(CompressionOutputStream.java:64)\n\tat org.apache.hadoop.io.compress.CompressorStream.close(CompressorStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:1015)\n\tat org.apache.spark.sql.catalyst.json.JacksonGenerator.close(JacksonGenerator.scala:274)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.close(JsonOutputWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:63)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:104)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:437)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1704)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:443)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2611)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\t... 43 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:607)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:456)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I\n\tat org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)\n\tat org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.compress(BlockCompressorStream.java:149)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.finish(BlockCompressorStream.java:142)\n\tat org.apache.hadoop.io.compress.CompressionOutputStream.close(CompressionOutputStream.java:64)\n\tat org.apache.hadoop.io.compress.CompressorStream.close(CompressorStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:1015)\n\tat org.apache.spark.sql.catalyst.json.JacksonGenerator.close(JacksonGenerator.scala:274)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.close(JsonOutputWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:63)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:104)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:437)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1704)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:443)\n\t... 19 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-447741929212383>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m (df.write.format(\"json\")\n\u001B[0m\u001B[1;32m      2\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m  \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"snappy\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m  .save(\"/tmp/data/json/df_json\"))\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    738\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    739\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 740\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    741\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    742\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o3738.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:603)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:359)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:198)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:124)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:209)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:356)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:306)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:183)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:427)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 204.0 failed 1 times, most recent failure: Lost task 2.0 in stage 204.0 (TID 355) (ip-10-172-200-232.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:607)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:456)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I\n\tat org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)\n\tat org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.compress(BlockCompressorStream.java:149)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.finish(BlockCompressorStream.java:142)\n\tat org.apache.hadoop.io.compress.CompressionOutputStream.close(CompressionOutputStream.java:64)\n\tat org.apache.hadoop.io.compress.CompressorStream.close(CompressorStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:1015)\n\tat org.apache.spark.sql.catalyst.json.JacksonGenerator.close(JacksonGenerator.scala:274)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.close(JsonOutputWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:63)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:104)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:437)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1704)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:443)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2611)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:324)\n\t... 43 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:607)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:456)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I\n\tat org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)\n\tat org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)\n\tat org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.compress(BlockCompressorStream.java:149)\n\tat org.apache.hadoop.io.compress.BlockCompressorStream.finish(BlockCompressorStream.java:142)\n\tat org.apache.hadoop.io.compress.CompressionOutputStream.close(CompressionOutputStream.java:64)\n\tat org.apache.hadoop.io.compress.CompressorStream.close(CompressorStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:1015)\n\tat org.apache.spark.sql.catalyst.json.JacksonGenerator.close(JacksonGenerator.scala:274)\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.close(JsonOutputWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:63)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:104)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:437)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1704)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:443)\n\t... 19 more\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87b3158b-d311-4264-8dab-b806c6497832"}}},{"cell_type":"markdown","source":["| Property name | Values | Meaning | Scope\n| --- | --- | --- | --- |\n| compression | none, uncompressed, bzip2, deflate, gzip, lz4, or snappy | Use this compression codec for writing. Note that read will only detect the compression or codec from the file extension.| Write |\n| dateFormat | yyyy-MM-dd or DateTimeFormatter | Use this format or any format from Java’s DateTime Formatter. | Read/ write |\n| multiLine | true, false | Use multiline mode. Default is false (single-line mode). | Read\n| allowUnquoted FieldNames | true, false | Allow unquoted JSON field names. Default is false. | Read"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fc836e7-f922-4470-8b43-ec5cce55b908"}}},{"cell_type":"markdown","source":["### CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b06a7c52-be7a-49f3-bb1a-84a6a1153f84"}}},{"cell_type":"markdown","source":["#### Leer de archivos CSV en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"494f0488-31d5-40ab-a89d-d8b7e2515d4a"}}},{"cell_type":"code","source":["file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\nschema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\ndf = (spark.read.format(\"csv\")\n .option(\"header\", \"true\")\n .schema(schema)\n .option(\"mode\", \"FAILFAST\") # Exit if any errors\n .option(\"nullValue\", \"\") # Replace any null data field with quotes\n .load(file))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d0dbb44-76f6-4f6f-9137-667d812aec31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos CSV en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41c168bc-4ef5-44f5-8d3e-7318f809c81c"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING csv\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n header \"true\",\n inferSchema \"true\",\n mode \"FAILFAST\"\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0bf1be4-b001-40e4-9006-12ec8966eee4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b655441-28dc-4493-9a62-cd58ee0d771f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab02755c-93fc-4bd0-85d4-f1d9993da764"}}},{"cell_type":"code","source":["df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d045c234-197f-4c56-9e28-e4020a62c42b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**\n## TABLA\nPágina 128"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24cb4914-68b0-470c-87f9-5e5d0a4351da"}}},{"cell_type":"markdown","source":["### AVRO"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d35d787e-4643-4f32-9d60-debd89feb6b0"}}},{"cell_type":"markdown","source":["#### Leer de archivos AVRO en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5913a7b5-c69b-4908-a0bf-eef380caafb1"}}},{"cell_type":"code","source":["df = (spark.read.format(\"avro\")\n .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"))\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e64eecad-600c-4bc2-ad67-d008437590ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------------------+-------------------+-----+\n|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n+--------------------------------+-------------------+-----+\n|United States                   |Romania            |1    |\n|United States                   |Ireland            |264  |\n|United States                   |India              |69   |\n|Egypt                           |United States      |24   |\n|Equatorial Guinea               |United States      |1    |\n|United States                   |Singapore          |25   |\n|United States                   |Grenada            |54   |\n|Costa Rica                      |United States      |477  |\n|Senegal                         |United States      |29   |\n|United States                   |Marshall Islands   |44   |\n|Guyana                          |United States      |17   |\n|United States                   |Sint Maarten       |53   |\n|Malta                           |United States      |1    |\n|Bolivia                         |United States      |46   |\n|Anguilla                        |United States      |21   |\n|Turks and Caicos Islands        |United States      |136  |\n|United States                   |Afghanistan        |2    |\n|Saint Vincent and the Grenadines|United States      |1    |\n|Italy                           |United States      |390  |\n|United States                   |Russia             |156  |\n+--------------------------------+-------------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------------------+-------------------+-----+\n|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n+--------------------------------+-------------------+-----+\n|United States                   |Romania            |1    |\n|United States                   |Ireland            |264  |\n|United States                   |India              |69   |\n|Egypt                           |United States      |24   |\n|Equatorial Guinea               |United States      |1    |\n|United States                   |Singapore          |25   |\n|United States                   |Grenada            |54   |\n|Costa Rica                      |United States      |477  |\n|Senegal                         |United States      |29   |\n|United States                   |Marshall Islands   |44   |\n|Guyana                          |United States      |17   |\n|United States                   |Sint Maarten       |53   |\n|Malta                           |United States      |1    |\n|Bolivia                         |United States      |46   |\n|Anguilla                        |United States      |21   |\n|Turks and Caicos Islands        |United States      |136  |\n|United States                   |Afghanistan        |2    |\n|Saint Vincent and the Grenadines|United States      |1    |\n|Italy                           |United States      |390  |\n|United States                   |Russia             |156  |\n+--------------------------------+-------------------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos AVRO en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ad2e6bd-bb53-43ff-b500-128194cd6645"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW episode_tbl\n USING avro\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"575644c5-969f-4999-8794-9cf4a0b7237c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM episode_tbl\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784f278a-bc55-4577-b60e-2df3521f69be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------------------+-------------------+-----+\n|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n+--------------------------------+-------------------+-----+\n|United States                   |Romania            |1    |\n|United States                   |Ireland            |264  |\n|United States                   |India              |69   |\n|Egypt                           |United States      |24   |\n|Equatorial Guinea               |United States      |1    |\n|United States                   |Singapore          |25   |\n|United States                   |Grenada            |54   |\n|Costa Rica                      |United States      |477  |\n|Senegal                         |United States      |29   |\n|United States                   |Marshall Islands   |44   |\n|Guyana                          |United States      |17   |\n|United States                   |Sint Maarten       |53   |\n|Malta                           |United States      |1    |\n|Bolivia                         |United States      |46   |\n|Anguilla                        |United States      |21   |\n|Turks and Caicos Islands        |United States      |136  |\n|United States                   |Afghanistan        |2    |\n|Saint Vincent and the Grenadines|United States      |1    |\n|Italy                           |United States      |390  |\n|United States                   |Russia             |156  |\n+--------------------------------+-------------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------------------+-------------------+-----+\n|DEST_COUNTRY_NAME               |ORIGIN_COUNTRY_NAME|count|\n+--------------------------------+-------------------+-----+\n|United States                   |Romania            |1    |\n|United States                   |Ireland            |264  |\n|United States                   |India              |69   |\n|Egypt                           |United States      |24   |\n|Equatorial Guinea               |United States      |1    |\n|United States                   |Singapore          |25   |\n|United States                   |Grenada            |54   |\n|Costa Rica                      |United States      |477  |\n|Senegal                         |United States      |29   |\n|United States                   |Marshall Islands   |44   |\n|Guyana                          |United States      |17   |\n|United States                   |Sint Maarten       |53   |\n|Malta                           |United States      |1    |\n|Bolivia                         |United States      |46   |\n|Anguilla                        |United States      |21   |\n|Turks and Caicos Islands        |United States      |136  |\n|United States                   |Afghanistan        |2    |\n|Saint Vincent and the Grenadines|United States      |1    |\n|Italy                           |United States      |390  |\n|United States                   |Russia             |156  |\n+--------------------------------+-------------------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos AVRO"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7130939-cc48-4ca4-a66c-cc3a96f87ca0"}}},{"cell_type":"code","source":["(df.write\n .format(\"avro\")\n .mode(\"overwrite\")\n .save(\"/tmp/data/avro/df_avro\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"965f2db7-9dd4-4ca7-81c2-c0a4e6e85e3d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10c0b527-c18c-4c35-8763-596df85b9362"}}},{"cell_type":"markdown","source":["##TABLA\nPágina 130"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24079f69-7de5-4d6b-8f6e-79e50f6152a8"}}},{"cell_type":"markdown","source":["### ORC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37673f46-984c-4f54-b798-c9e72100ecbb"}}},{"cell_type":"markdown","source":["#### Leer de archivos ORC en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75bf1d1d-ada2-4bb6-941b-e100b27132c5"}}},{"cell_type":"code","source":["file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\ndf = spark.read.format(\"orc\").option(\"path\", file).load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07b6f41e-4b1c-4b37-800e-31687fb19192"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos ORC en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a6479f3-6033-45f7-a9a4-0a7d02fca54e"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING orc\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72a474b8-1527-4763-b301-ad7e25aefb32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f931979a-a019-490e-b4c1-f63340941f54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos ORC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1274b9e6-f91c-4024-8686-5aeb10a47704"}}},{"cell_type":"code","source":["(df.write.format(\"orc\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/orc/flights_orc\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50fc95af-f95c-4a93-852a-1f497e703cef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Imágenes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"261dea74-4458-4414-aeef-70458ab224fe"}}},{"cell_type":"markdown","source":["#### Reading an image file into a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d61d7c2-5766-494a-90f3-826256858058"}}},{"cell_type":"code","source":["from pyspark.ml import image\nimage_dir = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\nimages_df = spark.read.format(\"image\").load(image_dir)\nimages_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffd11d4a-be7e-4b82-b907-b6152c257071"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nChannels: integer (nullable = true)\n |    |-- mode: integer (nullable = true)\n |    |-- data: binary (nullable = true)\n |-- label: integer (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nChannels: integer (nullable = true)\n |    |-- mode: integer (nullable = true)\n |    |-- data: binary (nullable = true)\n |-- label: integer (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Binary Files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"803eb944-a13b-4915-aef7-3ab5c3c89154"}}},{"cell_type":"markdown","source":["#### Reading a binary file into a DataFrame\nEl siguiente código lee todos los archivos JPG del directorio de entrada con cualquier directorio particionado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01fa660a-6310-4016-992d-9cecc2524bcc"}}},{"cell_type":"code","source":["path = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\nbinary_files_df = (spark.read.format(\"binaryFile\")\n .option(\"pathGlobFilter\", \"*.jpg\")\n .load(path))\nbinary_files_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b8b729c-897a-445d-b1fd-d67a71a0a224"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-------------------+------+--------------------+-----+\n|                path|   modificationTime|length|             content|label|\n+--------------------+-------------------+------+--------------------+-----+\n|dbfs:/databricks-...|2020-01-02 20:42:21| 55037|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:31| 54634|[FF D8 FF E0 00 1...|    1|\n|dbfs:/databricks-...|2020-01-02 20:42:21| 54624|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54505|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54475|[FF D8 FF E0 00 1...|    0|\n+--------------------+-------------------+------+--------------------+-----+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-------------------+------+--------------------+-----+\n|                path|   modificationTime|length|             content|label|\n+--------------------+-------------------+------+--------------------+-----+\n|dbfs:/databricks-...|2020-01-02 20:42:21| 55037|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:31| 54634|[FF D8 FF E0 00 1...|    1|\n|dbfs:/databricks-...|2020-01-02 20:42:21| 54624|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54505|[FF D8 FF E0 00 1...|    0|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54475|[FF D8 FF E0 00 1...|    0|\n+--------------------+-------------------+------+--------------------+-----+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Para ignorar la detección de datos de partición en un directorio, puede configurar ```recursiveFile Lookup to \"true\"```:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d39fb620-959a-4cfb-80e8-90f1c03fd0e6"}}},{"cell_type":"code","source":["binary_files_df = (spark.read.format(\"binaryFile\")\n .option(\"pathGlobFilter\", \"*.jpg\")\n .option(\"recursiveFileLookup\", \"true\")\n .load(path))\nbinary_files_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2fa6aa7-8d93-4071-bbf9-92e53ddab4c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|dbfs:/databricks-...|2020-01-02 20:42:21| 55037|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:31| 54634|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:21| 54624|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54505|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54475|[FF D8 FF E0 00 1...|\n+--------------------+-------------------+------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-------------------+------+--------------------+\n|                path|   modificationTime|length|             content|\n+--------------------+-------------------+------+--------------------+\n|dbfs:/databricks-...|2020-01-02 20:42:21| 55037|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:31| 54634|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:21| 54624|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54505|[FF D8 FF E0 00 1...|\n|dbfs:/databricks-...|2020-01-02 20:42:22| 54475|[FF D8 FF E0 00 1...|\n+--------------------+-------------------+------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["# APUNTES\nCon \\%fs puedo ejecutar comandos linux en el Notebook de Databricks. Se puede hacer un ls, ver directorios, crearlos y eliminarlos. En la siguiente web ```//https://docs.databricks.com/_static/notebooks/dbutils.html``` se observan todos los comandos disponibles.\n\n**Ejemplos**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28b51fe8-335e-4225-bd88-3ad1648a63e9"}}},{"cell_type":"code","source":["%fs\nls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f95d533-6de4-4080-8080-a93681c25b73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/Ejemplo_save_json.json/","Ejemplo_save_json.json/",0,0],["dbfs:/FileStore/","FileStore/",0,0],["dbfs:/databricks-datasets/","databricks-datasets/",0,0],["dbfs:/databricks-results/","databricks-results/",0,0],["dbfs:/learning-spark-v2/","learning-spark-v2/",0,0],["dbfs:/tmp/","tmp/",0,0],["dbfs:/user/","user/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Ejemplo_save_json.json/</td><td>Ejemplo_save_json.json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/</td><td>FileStore/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/</td><td>user/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nls /learning-spark-v2/sf-fire/Ejemplo_save_json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2559acc-0d7f-463d-bcb5-f433361eba94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS","_SUCCESS",0,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276","_committed_2238006556770501276",114,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276","_started_2238006556770501276",0,1651142576000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json","part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json",21353,1651142576000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276</td><td>_committed_2238006556770501276</td><td>114</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276</td><td>_started_2238006556770501276</td><td>0</td><td>1651142576000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>21353</td><td>1651142576000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nrm -r '/\"'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71c963aa-ff8d-4e52-b4ff-d1f27ab5fef5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res5: Boolean = false\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res5: Boolean = false\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter 4_Pyspark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2969547008920671}},"nbformat":4,"nbformat_minor":0}
