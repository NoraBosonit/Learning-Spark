{"cells":[{"cell_type":"markdown","source":["# Chapter 4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d588d54f-4b2f-4593-953b-444dcc2029fa"}}},{"cell_type":"markdown","source":["## Creamos una vista temporal"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfba51ea-8ffe-42ae-b325-03df373b63c6"}}},{"cell_type":"code","source":["%scala\nval schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d42be68e-7f39-44f9-a2b1-446a85bd8f2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">schema: String = date STRING, delay INT, distance INT, origin STRING, destination STRING\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">schema: String = date STRING, delay INT, distance INT, origin STRING, destination STRING\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// In Scala\nimport org.apache.spark.sql.SparkSession \nval spark = SparkSession\n .builder\n .appName(\"SparkSQLExampleApp\")\n .getOrCreate()\n// Path to data set \nval csvFile= \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n// Read and create a temporary view\n// Infer schema (note that for larger files you may want to specify the schema)\nval df = spark.read.format(\"csv\")\n .option(\"inferSchema\", \"true\")\n .option(\"header\", \"true\")\n .load(csvFile)\n// Create a temporary view\ndf.createOrReplaceTempView(\"us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a6909c5-aa7e-468e-bcb4-7e21b276e1a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"integer","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"distance","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">import org.apache.spark.sql.SparkSession\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5be5e8d7\ncsvFile: String = /databricks-datasets/learning-spark-v2/flights/departuredelays.csv\ndf: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.SparkSession\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5be5e8d7\ncsvFile: String = /databricks-datasets/learning-spark-v2/flights/departuredelays.csv\ndf: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\ndf.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"652bf49b-2ae3-4f25-af45-2af8127c38ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res2: Array[String] = Array(date, delay, distance, origin, destination)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Array[String] = Array(date, delay, distance, origin, destination)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Consultas SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c30a1a25-5b38-4df6-845c-52281d872c12"}}},{"cell_type":"markdown","source":["Buscamos todos los vuelos cuya distancia sea mayor a 1,000 millas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24bf6c0c-6bf3-48df-a7db-fa37b64679b2"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"SELECT distance, origin, destination \nFROM us_delay_flights_tbl WHERE distance > 1000 \nORDER BY distance DESC\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef87e83f-5bd2-4a2e-98b5-cd87fe7ba138"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+------+-----------+\ndistance|origin|destination|\n+--------+------+-----------+\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Todos los vuelos entre San Francisco (SFO) y Chicago (ORD) con al menos dos horas de retraso"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"685b11d7-1ddf-4f16-b7ae-31e0d0d8494b"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"SELECT date, delay, origin, destination \nFROM us_delay_flights_tbl \nWHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \nORDER by delay DESC\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4d19e55-999c-4dea-8479-66b878b10bea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-----+------+-----------+\n|   date|delay|origin|destination|\n+-------+-----+------+-----------+\n|2190925| 1638|   SFO|        ORD|\n|1031755|  396|   SFO|        ORD|\n|1022330|  326|   SFO|        ORD|\n|1051205|  320|   SFO|        ORD|\n|1190925|  297|   SFO|        ORD|\n|2171115|  296|   SFO|        ORD|\n|1071040|  279|   SFO|        ORD|\n|1051550|  274|   SFO|        ORD|\n|3120730|  266|   SFO|        ORD|\n|1261104|  258|   SFO|        ORD|\n+-------+-----+------+-----------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+-----------+\n   date|delay|origin|destination|\n+-------+-----+------+-----------+\n2190925| 1638|   SFO|        ORD|\n1031755|  396|   SFO|        ORD|\n1022330|  326|   SFO|        ORD|\n1051205|  320|   SFO|        ORD|\n1190925|  297|   SFO|        ORD|\n2171115|  296|   SFO|        ORD|\n1071040|  279|   SFO|        ORD|\n1051550|  274|   SFO|        ORD|\n3120730|  266|   SFO|        ORD|\n1261104|  258|   SFO|        ORD|\n+-------+-----+------+-----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Etiquetar todos los vuelos de EE. UU., independientemente de su origen y destino, con una indicaci√≥n de los retrasos que experimentaron: retrasos muy largos (> 6 horas), retrasos largos (2 a 6 horas), etc. Agregaremos estas etiquetas legibles por humanos en una nueva columna llamada Flight_Delays"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eabeff00-00c7-4a9c-8629-ab56c0bae84f"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"SELECT delay, origin, destination,\n CASE\n WHEN delay > 360 THEN 'Very Long Delays'\n WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n WHEN delay = 0 THEN 'No Delays'\n ELSE 'Early'\n END AS Flight_Delays\n FROM us_delay_flights_tbl\n ORDER BY origin, delay DESC\"\"\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16f7ae8e-83f0-4407-92e8-524eb0836c05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+------+-----------+-------------+\n|delay|origin|destination|Flight_Delays|\n+-----+------+-----------+-------------+\n|  333|   ABE|        ATL|  Long Delays|\n|  305|   ABE|        ATL|  Long Delays|\n|  275|   ABE|        ATL|  Long Delays|\n|  257|   ABE|        ATL|  Long Delays|\n|  247|   ABE|        DTW|  Long Delays|\n|  247|   ABE|        ATL|  Long Delays|\n|  219|   ABE|        ORD|  Long Delays|\n|  211|   ABE|        ATL|  Long Delays|\n|  197|   ABE|        DTW|  Long Delays|\n|  192|   ABE|        ORD|  Long Delays|\n+-----+------+-----------+-------------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+-----------+-------------+\ndelay|origin|destination|Flight_Delays|\n+-----+------+-----------+-------------+\n  333|   ABE|        ATL|  Long Delays|\n  305|   ABE|        ATL|  Long Delays|\n  275|   ABE|        ATL|  Long Delays|\n  257|   ABE|        ATL|  Long Delays|\n  247|   ABE|        DTW|  Long Delays|\n  247|   ABE|        ATL|  Long Delays|\n  219|   ABE|        ORD|  Long Delays|\n  211|   ABE|        ATL|  Long Delays|\n  197|   ABE|        DTW|  Long Delays|\n  192|   ABE|        ORD|  Long Delays|\n+-----+------+-----------+-------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Ejercicio: Convertir la columna fecha a un formato legible**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9390c3b-e0c4-4ed9-8a5f-c2596a42edf1"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\nval df_new = (df\n         .withColumn(\"DateAndHour\", to_timestamp(col(\"date\"), \"mm-dd hh:mm\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"764f6f5e-57e5-49dd-8929-2d35da3fb059"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_new","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"distance","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}},{"name":"DateAndHour","type":"timestamp","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">import org.apache.spark.sql.functions._\ndf_new: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.functions._\ndf_new: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\ndef toDateFormatUDF(dStr:String) : String  = {\n  return s\"${dStr(0)}${dStr(1)}${'-'}${dStr(2)}${dStr(3)}${' '}${dStr(4)}${dStr(5)}${':'}${dStr(6)}${dStr(7)}\"\n}\n\n// test  it\ntoDateFormatUDF(\"02190925\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ea02035-8da7-4574-8e7d-8c2d00e15116"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">toDateFormatUDF: (dStr: String)String\nres2: String = 02-19 09:25\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">toDateFormatUDF: (dStr: String)String\nres2: String = 02-19 09:25\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\ndf.selectExpr(\"toDateFormatUDF(date) as data_format\").show(10, false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b08fe3c5-7970-4f21-bae1-367613910fed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\">\tat org.apache.spark.sql.errors.QueryCompilationErrors$.noSuchFunctionError(QueryCompilationErrors.scala:1794)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$24.applyOrElse(Analyzer.scala:2750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$24.applyOrElse(Analyzer.scala:2736)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:497)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1143)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1142)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:497)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:161)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:202)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:218)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:218)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:161)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:132)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:246)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:243)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2730)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:262)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:262)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:93)\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3964)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1533)\n\tat org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1567)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:1)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:52)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:54)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:56)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:58)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:60)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:62)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:64)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw.&lt;init&gt;(command-2944212098499367:66)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw.&lt;init&gt;(command-2944212098499367:68)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read.&lt;init&gt;(command-2944212098499367:70)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$.&lt;init&gt;(command-2944212098499367:74)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$.&lt;clinit&gt;(command-2944212098499367)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:968)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:921)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)</div>","errorSummary":"AnalysisException: Undefined function: toDateFormatUDF. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.todateformatudf.; line 1 pos 0","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat org.apache.spark.sql.errors.QueryCompilationErrors$.noSuchFunctionError(QueryCompilationErrors.scala:1794)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$24.applyOrElse(Analyzer.scala:2750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$24.applyOrElse(Analyzer.scala:2736)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:492)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:497)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1143)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1142)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:497)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:161)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:202)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:202)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:218)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:218)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:161)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:132)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:246)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsWithPruning$1.applyOrElse(AnalysisHelper.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:243)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2736)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2730)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:262)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:262)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:93)\n\tat org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3964)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1533)\n\tat org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1567)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:1)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:52)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:54)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:56)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:58)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:60)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:62)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw$$iw.&lt;init&gt;(command-2944212098499367:64)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw$$iw.&lt;init&gt;(command-2944212098499367:66)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$$iw.&lt;init&gt;(command-2944212098499367:68)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read.&lt;init&gt;(command-2944212098499367:70)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$.&lt;init&gt;(command-2944212098499367:74)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$read$.&lt;clinit&gt;(command-2944212098499367)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line1d19146407004c489d7f45a5f2f3d1e273.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:968)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:921)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:602)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:579)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.udf.register(\"toDateFormatUDF\", toDateFormatUDF(_:String):String)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9bb0ed8-c125-4725-b2f5-15d98e680714"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res61: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$9266/548813369@3cb1151a,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(toDateFormatUDF),true,true)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res61: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$9266/548813369@3cb1151a,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),Some(toDateFormatUDF),true,true)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nval df_new = df.selectExpr(\"toDateFormatUDF(date) as DateAndHour\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c296c40e-46aa-44a8-8b89-e36f06a431da"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_new","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"DateAndHour","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">df_new: org.apache.spark.sql.DataFrame = [DateAndHour: string]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df_new: org.apache.spark.sql.DataFrame = [DateAndHour: string]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\ndf_new.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb370c67-1c21-41db-9596-4ee07e2677fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+-----+--------+------+-----------+-----------+\n|    date|delay|distance|origin|destination|DateAndHour|\n+--------+-----+--------+------+-----------+-----------+\n|01011245|    6|     602|   ABE|        ATL|       null|\n|01020600|   -8|     369|   ABE|        DTW|       null|\n|01021245|   -2|     602|   ABE|        ATL|       null|\n|01020605|   -4|     602|   ABE|        ATL|       null|\n|01031245|   -4|     602|   ABE|        ATL|       null|\n|01030605|    0|     602|   ABE|        ATL|       null|\n|01041243|   10|     602|   ABE|        ATL|       null|\n|01040605|   28|     602|   ABE|        ATL|       null|\n|01051245|   88|     602|   ABE|        ATL|       null|\n|01050605|    9|     602|   ABE|        ATL|       null|\n|01061215|   -6|     602|   ABE|        ATL|       null|\n|01061725|   69|     602|   ABE|        ATL|       null|\n|01061230|    0|     369|   ABE|        DTW|       null|\n|01060625|   -3|     602|   ABE|        ATL|       null|\n|01070600|    0|     369|   ABE|        DTW|       null|\n|01071725|    0|     602|   ABE|        ATL|       null|\n|01071230|    0|     369|   ABE|        DTW|       null|\n|01070625|    0|     602|   ABE|        ATL|       null|\n|01071219|    0|     569|   ABE|        ORD|       null|\n|01080600|    0|     369|   ABE|        DTW|       null|\n+--------+-----+--------+------+-----------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----+--------+------+-----------+-----------+\n    date|delay|distance|origin|destination|DateAndHour|\n+--------+-----+--------+------+-----------+-----------+\n01011245|    6|     602|   ABE|        ATL|       null|\n01020600|   -8|     369|   ABE|        DTW|       null|\n01021245|   -2|     602|   ABE|        ATL|       null|\n01020605|   -4|     602|   ABE|        ATL|       null|\n01031245|   -4|     602|   ABE|        ATL|       null|\n01030605|    0|     602|   ABE|        ATL|       null|\n01041243|   10|     602|   ABE|        ATL|       null|\n01040605|   28|     602|   ABE|        ATL|       null|\n01051245|   88|     602|   ABE|        ATL|       null|\n01050605|    9|     602|   ABE|        ATL|       null|\n01061215|   -6|     602|   ABE|        ATL|       null|\n01061725|   69|     602|   ABE|        ATL|       null|\n01061230|    0|     369|   ABE|        DTW|       null|\n01060625|   -3|     602|   ABE|        ATL|       null|\n01070600|    0|     369|   ABE|        DTW|       null|\n01071725|    0|     602|   ABE|        ATL|       null|\n01071230|    0|     369|   ABE|        DTW|       null|\n01070625|    0|     602|   ABE|        ATL|       null|\n01071219|    0|     569|   ABE|        ORD|       null|\n01080600|    0|     369|   ABE|        DTW|       null|\n+--------+-----+--------+------+-----------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Las consultas anteriores se pueden expresar como consultas API. Por ejemplo la primera:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16fd6da8-0471-4792-8767-2c220a0f3a1c"}}},{"cell_type":"code","source":["%scala\n(df.select(\"distance\", \"origin\", \"destination\")\n .where(col(\"distance\") > 1000)\n .orderBy(desc(\"distance\"))).show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56d5bef8-92ba-4bff-afb7-afd035d0234b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+------+-----------+\n|distance|origin|destination|\n+--------+------+-----------+\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n|    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+------+-----------+\ndistance|origin|destination|\n+--------+------+-----------+\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n    4330|   HNL|        JFK|\n+--------+------+-----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["La segunda"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01590b9e-0aa5-46ed-8def-42f022267745"}}},{"cell_type":"code","source":["%scala\n//spark.sql(\"\"\"SELECT date, delay, origin, destination \n//FROM us_delay_flights_tbl \n//WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n//ORDER by delay DESC\"\"\").show(10)\n\ndf\n       .select(\"date\", \"delay\", \"origin\", \"destination\")\n       .where(col(\"delay\") > 120 && col(\"origin\") === \"SFO\" && col(\"destination\") === \"ORD\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f082ae5a-6b83-468a-95d3-ce956166b2a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+-----+------+-----------+\n|    date|delay|origin|destination|\n+--------+-----+------+-----------+\n|01011410|  124|   SFO|        ORD|\n|01022330|  326|   SFO|        ORD|\n|01021410|  190|   SFO|        ORD|\n|01101410|  184|   SFO|        ORD|\n|01190925|  297|   SFO|        ORD|\n|01241110|  139|   SFO|        ORD|\n|01301800|  167|   SFO|        ORD|\n|01011237|  122|   SFO|        ORD|\n|01032258|  163|   SFO|        ORD|\n|01031920|  193|   SFO|        ORD|\n|01031755|  396|   SFO|        ORD|\n|01071040|  279|   SFO|        ORD|\n|01161210|  225|   SFO|        ORD|\n|01221040|  215|   SFO|        ORD|\n|01261104|  258|   SFO|        ORD|\n|01020720|  145|   SFO|        ORD|\n|01021205|  154|   SFO|        ORD|\n|01031550|  131|   SFO|        ORD|\n|01041205|  126|   SFO|        ORD|\n|01051205|  320|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----+------+-----------+\n    date|delay|origin|destination|\n+--------+-----+------+-----------+\n01011410|  124|   SFO|        ORD|\n01022330|  326|   SFO|        ORD|\n01021410|  190|   SFO|        ORD|\n01101410|  184|   SFO|        ORD|\n01190925|  297|   SFO|        ORD|\n01241110|  139|   SFO|        ORD|\n01301800|  167|   SFO|        ORD|\n01011237|  122|   SFO|        ORD|\n01032258|  163|   SFO|        ORD|\n01031920|  193|   SFO|        ORD|\n01031755|  396|   SFO|        ORD|\n01071040|  279|   SFO|        ORD|\n01161210|  225|   SFO|        ORD|\n01221040|  215|   SFO|        ORD|\n01261104|  258|   SFO|        ORD|\n01020720|  145|   SFO|        ORD|\n01021205|  154|   SFO|        ORD|\n01031550|  131|   SFO|        ORD|\n01041205|  126|   SFO|        ORD|\n01051205|  320|   SFO|        ORD|\n+--------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["La tercera"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"470bae6b-9499-41c7-a0d7-5a504590a017"}}},{"cell_type":"code","source":["%scala\n//spark.sql(\"\"\"SELECT delay, origin, destination,\n//CASE\n//WHEN delay > 360 THEN 'Very Long Delays'\n//WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n//WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n//WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n//WHEN delay = 0 THEN 'No Delays'\n//ELSE 'Early'\n//END AS Flight_Delays\n//FROM us_delay_flights_tbl\n//ORDER BY origin, delay DESC\"\"\").show(10)\n\n\nval df_3 = df.withColumn(\"Flight_Delays\",\n                        when(col(\"delay\") > 360,\n                            lit(\"Very Long Delays\"))\n                        when(col(\"delay\") > 120 && col(\"delay\") < 360,\n                            lit(\"Long Delays\"))\n                        when(col(\"delay\") > 60 && col(\"delay\") < 120,\n                            lit(\"Short Delays\"))\n                        when(col(\"delay\") > 0 && col(\"delay\") < 60,\n                            lit(\"Tolerable Delays\"))\n                        when(col(\"delay\") === 0,\n                            lit(\"No Delays\"))\n                        otherwise(lit(\"Early\")))\n.orderBy(desc(\"origin\"))\n.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fc2e602-c165-4902-b061-b8ae7708bd0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+-----+--------+------+-----------+----------------+\n|    date|delay|distance|origin|destination|   Flight_Delays|\n+--------+-----+--------+------+-----------+----------------+\n|03010941|   -2|     139|   YUM|        PHX|           Early|\n|03021130|    9|     139|   YUM|        PHX|Tolerable Delays|\n|03011255|   -9|     139|   YUM|        PHX|           Early|\n|03011130|   -7|     139|   YUM|        PHX|           Early|\n|03010800|   -6|     139|   YUM|        PHX|           Early|\n|03010600|    5|     206|   YUM|        LAX|Tolerable Delays|\n|03011530|   -6|     206|   YUM|        LAX|           Early|\n|03020941|  -10|     139|   YUM|        PHX|           Early|\n|03021255|   -5|     139|   YUM|        PHX|           Early|\n|03021859|  -10|     139|   YUM|        PHX|           Early|\n+--------+-----+--------+------+-----------+----------------+\nonly showing top 10 rows\n\ndf_3: Unit = ()\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----+--------+------+-----------+----------------+\n    date|delay|distance|origin|destination|   Flight_Delays|\n+--------+-----+--------+------+-----------+----------------+\n03010941|   -2|     139|   YUM|        PHX|           Early|\n03021130|    9|     139|   YUM|        PHX|Tolerable Delays|\n03011255|   -9|     139|   YUM|        PHX|           Early|\n03011130|   -7|     139|   YUM|        PHX|           Early|\n03010800|   -6|     139|   YUM|        PHX|           Early|\n03010600|    5|     206|   YUM|        LAX|Tolerable Delays|\n03011530|   -6|     206|   YUM|        LAX|           Early|\n03020941|  -10|     139|   YUM|        PHX|           Early|\n03021255|   -5|     139|   YUM|        PHX|           Early|\n03021859|  -10|     139|   YUM|        PHX|           Early|\n+--------+-----+--------+------+-----------+----------------+\nonly showing top 10 rows\n\ndf_3: Unit = ()\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Creando Tablas y Databases de SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f321906d-283f-4728-bbeb-0dc200a78ea4"}}},{"cell_type":"markdown","source":["### Databases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"773260fd-50e6-48f8-8ca0-a0bf4502d0ba"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"CREATE DATABASE learn_spark_db\")\nspark.sql(\"USE learn_spark_db\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86c403d5-da9d-4aa4-992e-5a373b3b1960"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res17","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res17: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res17: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ahora todas las tablas que se creen estar√°n dentro de esa base de datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db62d166-e637-4fc6-9d90-a7c4d6f500ef"}}},{"cell_type":"markdown","source":["d\n### Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a17809a-32dd-4bca-acfa-1c1209749c8f"}}},{"cell_type":"markdown","source":["#### Managed\nLas tablas administradas administran tanto los metadatos como los datos. Si se ejecuta un DROP se eliminar√≠an los datos reales y los metadatos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"390f7833-3478-4bfb-a1f7-c1052e507b3a"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da9b6e7c-6139-48b2-87ab-ea806cfd3453"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">++\n||\n++\n++\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">++\n|\n++\n++\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Aunque cierre un cluster y abra otro, lo que he hecho en sesiones anteriores se guarda en el fichero de metadatos. Al abrir otra sesi√≥n, la tabla no aparece en el contenido de las tablas, pero no me deja volverla a crear porque ya existen los metadatos de la misma. Tendr√© que eliminar el registro"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5bf54c8-137c-407a-8af0-63501b5b5e24"}}},{"cell_type":"code","source":["%scala\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/learn_spark_db.db/managed_us_delay_flights_tbl\",recurse=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"923129d7-2e46-4128-a1b2-9303393e9953"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res20: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res20: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Unmanaged\nEn las tablas no administradas, Spark solo administra los metadatos mientras que el propio usuario maneja los datos. Aqu√≠ el DROP solo eliminar√≠a los metadatos."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe929357-984c-4f35-9f59-238659cff5a4"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n distance INT, origin STRING, destination STRING) \n USING csv OPTIONS (PATH \n '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d7693e7-7a89-4ebb-9958-e3abb9c9a8ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res22","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res22: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res22: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Creando vistas\nLas vistas se crean sobre tablas existentes y pueden ser globales o session-scoped. Las globales son visibles en todas las Spark-Sessions de un cluster y la segunda solo es visible para una sola Spark-session (son temporales, desaparecen despu√©s de la finalizaci√≥n de la Spark Application."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e365ac0-92ae-494c-9bc3-4ed56c697e32"}}},{"cell_type":"markdown","source":["#### Global\nSon visibles en todos los SparkSession en un cluster dado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e529e853-fe76-4158-924e-8fd3650e9e59"}}},{"cell_type":"code","source":["%scala\n//A partir de una tabla existente: SQL\nspark.sql(\"\"\"CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n origin = 'SFO'\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b6bfe35-1945-4884-84a2-ab7e97335ec7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res24","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res24: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res24: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// API\nval df_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM \n us_delay_flights_tbl WHERE origin = 'SFO'\"\"\")\ndf_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\ndf_sfo.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4736fb3-0038-4027-bc8f-46f515de2327"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_sfo","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"integer","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------+-----+------+-----------+\n|   date|delay|origin|destination|\n+-------+-----+------+-----------+\n|1011250|   55|   SFO|        JFK|\n|1012230|    0|   SFO|        JFK|\n|1010705|   -7|   SFO|        JFK|\n|1010620|   -3|   SFO|        MIA|\n|1010915|   -3|   SFO|        LAX|\n+-------+-----+------+-----------+\nonly showing top 5 rows\n\ndf_sfo: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+-----------+\n   date|delay|origin|destination|\n+-------+-----+------+-----------+\n1011250|   55|   SFO|        JFK|\n1012230|    0|   SFO|        JFK|\n1010705|   -7|   SFO|        JFK|\n1010620|   -3|   SFO|        MIA|\n1010915|   -3|   SFO|        LAX|\n+-------+-----+------+-----------+\nonly showing top 5 rows\n\ndf_sfo: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7afba934-f806-40ef-b05d-2ab9ff349e9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-----+------+-----------+\n|   date|delay|origin|destination|\n+-------+-----+------+-----------+\n|1011250|   55|   SFO|        JFK|\n|1012230|    0|   SFO|        JFK|\n|1010705|   -7|   SFO|        JFK|\n|1010620|   -3|   SFO|        MIA|\n|1010915|   -3|   SFO|        LAX|\n|1011005|   -8|   SFO|        DFW|\n|1011800|    0|   SFO|        ORD|\n|1011740|   -7|   SFO|        LAX|\n|1012015|   -7|   SFO|        LAX|\n|1012110|   -1|   SFO|        MIA|\n|1011610|  134|   SFO|        DFW|\n|1011240|   -6|   SFO|        MIA|\n|1010755|   -3|   SFO|        DFW|\n|1010020|    0|   SFO|        DFW|\n|1010705|   -6|   SFO|        LAX|\n|1010925|   -3|   SFO|        ORD|\n|1010555|   -6|   SFO|        ORD|\n|1011105|   -8|   SFO|        DFW|\n|1012330|   32|   SFO|        ORD|\n|1011330|    3|   SFO|        DFW|\n+-------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+-----------+\n   date|delay|origin|destination|\n+-------+-----+------+-----------+\n1011250|   55|   SFO|        JFK|\n1012230|    0|   SFO|        JFK|\n1010705|   -7|   SFO|        JFK|\n1010620|   -3|   SFO|        MIA|\n1010915|   -3|   SFO|        LAX|\n1011005|   -8|   SFO|        DFW|\n1011800|    0|   SFO|        ORD|\n1011740|   -7|   SFO|        LAX|\n1012015|   -7|   SFO|        LAX|\n1012110|   -1|   SFO|        MIA|\n1011610|  134|   SFO|        DFW|\n1011240|   -6|   SFO|        MIA|\n1010755|   -3|   SFO|        DFW|\n1010020|    0|   SFO|        DFW|\n1010705|   -6|   SFO|        LAX|\n1010925|   -3|   SFO|        ORD|\n1010555|   -6|   SFO|        ORD|\n1011105|   -8|   SFO|        DFW|\n1012330|   32|   SFO|        ORD|\n1011330|    3|   SFO|        DFW|\n+-------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Eliminar vistas**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"695083d2-be7c-4cb2-bf85-920fdd7a1d5c"}}},{"cell_type":"code","source":["%scala\nspark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8498f67d-a8f9-4401-963c-62820e826107"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res67: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res67: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Temporal\nLas vistas son visibles para un √∫nico SparkSessios y se eliminan despu√©s de finalizar la aplicaci√≥n Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2c78c9e-11b4-4dc4-a9b1-4a416f2e5666"}}},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n origin = 'JFK'\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed5abed-f38e-4fea-a679-432b681df2a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res33","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res33: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res33: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n//API\nval df_jfk = spark.sql(\"\"\"SELECT date, delay, origin, destination FROM \n us_delay_flights_tbl WHERE origin = 'JFK'\"\"\")\ndf_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\ndf_jfk.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53d73cbb-6090-43ac-bb60-791943a3c396"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_jfk","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"integer","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------+-----+------+-----------+\n|   date|delay|origin|destination|\n+-------+-----+------+-----------+\n|1010900|   14|   JFK|        LAX|\n|1011200|   -3|   JFK|        LAX|\n|1011900|    2|   JFK|        LAX|\n|1011700|   11|   JFK|        LAS|\n|1010800|   -1|   JFK|        SFO|\n+-------+-----+------+-----------+\nonly showing top 5 rows\n\ndf_jfk: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+-----------+\n   date|delay|origin|destination|\n+-------+-----+------+-----------+\n1010900|   14|   JFK|        LAX|\n1011200|   -3|   JFK|        LAX|\n1011900|    2|   JFK|        LAX|\n1011700|   11|   JFK|        LAS|\n1010800|   -1|   JFK|        SFO|\n+-------+-----+------+-----------+\nonly showing top 5 rows\n\ndf_jfk: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 2 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"SELECT * FROM us_origin_airport_JFK_tmp_view\"\"\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76ce42e5-0bec-4e35-8c1a-c1261c690a3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-----+------+-----------+\n|   date|delay|origin|destination|\n+-------+-----+------+-----------+\n|1010900|   14|   JFK|        LAX|\n|1011200|   -3|   JFK|        LAX|\n|1011900|    2|   JFK|        LAX|\n|1011700|   11|   JFK|        LAS|\n|1010800|   -1|   JFK|        SFO|\n|1011540|   -4|   JFK|        DFW|\n|1011705|    5|   JFK|        SAN|\n|1011530|   -3|   JFK|        SFO|\n|1011630|   -3|   JFK|        SJU|\n|1011345|    2|   JFK|        LAX|\n|1011545|   -3|   JFK|        LAX|\n|1011510|   -1|   JFK|        MIA|\n|1011745|    7|   JFK|        SFO|\n|1011250|    3|   JFK|        BOS|\n|1011645|  142|   JFK|        LAX|\n|1012135|   -2|   JFK|        LAX|\n|1011715|   18|   JFK|        ORD|\n|1011615|   25|   JFK|        IAH|\n|1011850|   -2|   JFK|        SEA|\n|1011725|   -5|   JFK|        BOS|\n+-------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+-----------+\n   date|delay|origin|destination|\n+-------+-----+------+-----------+\n1010900|   14|   JFK|        LAX|\n1011200|   -3|   JFK|        LAX|\n1011900|    2|   JFK|        LAX|\n1011700|   11|   JFK|        LAS|\n1010800|   -1|   JFK|        SFO|\n1011540|   -4|   JFK|        DFW|\n1011705|    5|   JFK|        SAN|\n1011530|   -3|   JFK|        SFO|\n1011630|   -3|   JFK|        SJU|\n1011345|    2|   JFK|        LAX|\n1011545|   -3|   JFK|        LAX|\n1011510|   -1|   JFK|        MIA|\n1011745|    7|   JFK|        SFO|\n1011250|    3|   JFK|        BOS|\n1011645|  142|   JFK|        LAX|\n1012135|   -2|   JFK|        LAX|\n1011715|   18|   JFK|        ORD|\n1011615|   25|   JFK|        IAH|\n1011850|   -2|   JFK|        SEA|\n1011725|   -5|   JFK|        BOS|\n+-------+-----+------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Eliminar vistas**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"827a10aa-dfd9-40ca-a000-d221051f1faf"}}},{"cell_type":"code","source":["%scala\nspark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6643fbf5-ebf6-436b-9943-0a1a5dc99dcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res68: Boolean = true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res68: Boolean = true\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Metadatos\nSpark administra los metadatos asociados a cada tabla, ya sea administrada o no administrada. Para administrar los metadatos se utiliza Catalog la cual es una herramienta de alto nivel de Spark SQL para almacenar metadatos.\n\nDespu√©s de crear la vartiable de la SparkSession, se puede acceder al metadata almacenado de la siaguiente manera:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b76e7879-d314-45fb-aae2-f3409ad474a4"}}},{"cell_type":"code","source":["%scala\ndisplay(spark.catalog.listTables(dbName=\"global_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40fc3792-bb55-4cac-8200-a758b3d820eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["us_origin_airport_sfo_global_tmp_view","global_temp",null,"TEMPORARY",true],["us_delay_flights_tbl",null,null,"TEMPORARY",true],["us_origin_airport_jfk_tmp_view",null,null,"TEMPORARY",true]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"database","type":"\"string\"","metadata":"{}"},{"name":"description","type":"\"string\"","metadata":"{}"},{"name":"tableType","type":"\"string\"","metadata":"{}"},{"name":"isTemporary","type":"\"boolean\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>database</th><th>description</th><th>tableType</th><th>isTemporary</th></tr></thead><tbody><tr><td>us_origin_airport_sfo_global_tmp_view</td><td>global_temp</td><td>null</td><td>TEMPORARY</td><td>true</td></tr><tr><td>us_delay_flights_tbl</td><td>null</td><td>null</td><td>TEMPORARY</td><td>true</td></tr><tr><td>us_origin_airport_jfk_tmp_view</td><td>null</td><td>null</td><td>TEMPORARY</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Almacenamiento en cach√© de tablas SQL**\n\nSe puede cache y uncahe tablas SQL y vistas. Adem√°s, si especificas la tabla como LAZY, se guardar√° en cach√© cuando se utiliza la tabla por primera vez y no inmediatamente cuando se crea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e8d49e1-a5b3-4850-ad5b-32bfbaabd9af"}}},{"cell_type":"code","source":["%scala\n//In SQL\nspark.sql(\"CACHE [LAZY] TABLE <table-name>\")\nspark.sql(\"UNCACHE TABLE <table-name>\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a64d5ec9-1faf-481e-ac9c-f03ede111319"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Leyendo tablas a DataFrames\nSi ya hay una base de datos learn_spark_db y una tabla us_delay_flights_tbl preparadas para ser utilizadas. En vez de importar datos directamente del JSON file para tener un dataframe, podemos simplemente ejecutar una consulta SQL a la tabla y asignarle como resultado un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"233f3f4e-16ea-4d7f-89d4-04568d457551"}}},{"cell_type":"code","source":["%scala\n//Dos formas\nval usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\nval usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")\nusFlightsDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9e8c068-31d2-4f88-9980-62e0cb473345"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"usFlightsDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"distance","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"usFlightsDF2","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"delay","type":"integer","nullable":true,"metadata":{}},{"name":"distance","type":"integer","nullable":true,"metadata":{}},{"name":"origin","type":"string","nullable":true,"metadata":{}},{"name":"destination","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">usFlightsDF: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\nusFlightsDF2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">usFlightsDF: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\nusFlightsDF2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data Sources for DataFrames and SQL Tables\nSpark SQL proporciona una gran variedad de data sources. Adem√°s de proporcionar un conjunto de m√©todos reales para leer y escribir desde estas fuentes utilizando la Data Sources API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4b86667-b75d-4152-a2b7-5ced582eef7b"}}},{"cell_type":"markdown","source":["### DataFrameReader\nEs la herramienta con la que se lee una fuente de datos en un DataFrame. Tiene un formato definido y un patr√≥n recomendado de uso.\n\n```DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7b6da32-c3ea-4d5f-a8eb-36114ab7f183"}}},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c120480-f2b6-45aa-8898-657ab6aed680"}}},{"cell_type":"markdown","source":["| Method | Arguments | Description |\n| --- | --- | --- |\n| format() | \"parquet\", \"csv\", \"txt\", \"json\",\"jdbc\", \"orc\", \"avro\", etc. | If you don‚Äôt specify this method, then the default ismParquet or whatever is set in spark.sql.sources.default.|\n| option() |(\"mode\", {PERMISSIVE / FAILFAST / DROPMALFORMED } )(\"inferSchema\", {true / false}) (\"path\", \"path_file_data_source\") | A series of key/value pairs and options. The Spark documentation shows some examples and explains the different modes and their actions. The default mode is PERMISSIVE. The \"inferSchema\" and \"mode\" options are specific to the JSON and CSV file formats.|\n| schema() | DDL String or StructType, e.g., 'A INT, B STRING' orStructType(...) | For JSON or CSV format, you can specify to infer the schema in the option() method. Generally, providing a schema for any format makes loading faster and ensures your data conforms to the expected schema.|\n| load() | \"/path/to/data/source\" | The path to the data source. This can be empty if specified in option(\"path\", \"...\").|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"349e8720-4c1c-45f9-ae6d-4568554a8a1b"}}},{"cell_type":"markdown","source":["**Ejemplos**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"487fd733-d8fd-40e5-a614-8286f9037ad9"}}},{"cell_type":"code","source":["%scala\n// Use Parquet \nval file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\"\"\nval df = spark.read.format(\"parquet\").load(file)\n// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n//val df2 = spark.read.load(file)\n// Use CSV\nval df3 = spark.read.format(\"csv\")\n .option(\"inferSchema\", \"true\")\n .option(\"header\", \"true\")\n .option(\"mode\", \"PERMISSIVE\")\n .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\")\n// Use JSON\nval df4 = spark.read.format(\"json\")\n .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\n\ndf.show(5)\ndf3.show(5)\ndf4.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0331dcad-f135-4e66-bfd8-fd050ef10673"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"DEST_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"ORIGIN_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"count","type":"long","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"df3","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"DEST_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"ORIGIN_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"count","type":"integer","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"df4","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"DEST_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"ORIGIN_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"count","type":"long","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nfile: String = /databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\ndf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\ndf3: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\ndf4: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|    1|\n    United States|            Ireland|  264|\n    United States|              India|   69|\n            Egypt|      United States|   24|\nEquatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|    1|\n    United States|            Ireland|  264|\n    United States|              India|   69|\n            Egypt|      United States|   24|\nEquatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|   15|\n    United States|            Croatia|    1|\n    United States|            Ireland|  344|\n            Egypt|      United States|   15|\n    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nfile: String = /databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\ndf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\ndf3: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\ndf4: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["En general, no se necesita ning√∫n esquema cuando se lee desde una fuente de datos est√°tica de Parquet; los metadatos de Parquet generalmente contienen el esquema,  por lo que se deduce."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34e8f773-473a-4fa0-9c96-4e7ae058a921"}}},{"cell_type":"markdown","source":["### DataFrameWriter\nGuarda o escribe datos en una fuente de datos integrada especificada y estos son sus patrones de uso:\n```\nDataFrameWriter.format(args)\n .option(args)\n .bucketBy(args)\n .partitionBy(args)\n .save(path)\n```\no\n\n```\nDataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a593de0b-805b-46ef-9a12-b6d50d63805e"}}},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0d46406-ef8d-43e5-8829-2cfb00f4b58b"}}},{"cell_type":"markdown","source":["| Method | Arguments | Description |\n| --- | --- | --- |\n| format() | \"parquet\", \"csv\", \"txt\", \"json\",\"jdbc\", \"orc\", \"avro\", etc. | If you don‚Äôt specify this method, then the default is Parquet or whatever is set in spark.sql.sources.default.|\n| option() | (\"mode\", {append / overwrite / ignore / error or errorifexists} ) (\"mode\", {SaveMode.Overwrite / SaveMode.Append, Save Mode.Ignore, SaveMode.ErrorIfExists}) (\"path\", \"path_to_write_to\") | A series of key/value pairs and options. The Spark documentation shows some examples. This is an overloaded method. The default mode options are error or error ifexists and SaveMode.ErrorIfExists; they throw an exception at runtime if the data already exists.|\n| bucketBy() | (numBuckets, col, col..., coln) | The number of buckets and names of columns to bucket by. Uses Hive‚Äôs bucketing scheme on a filesystem.|\n| save() | \"/path/to/data/source\" | The path to save to. This can be empty if specified in option(\"path\", \"...\"). |\n| saveAsTable() | \"table_name\" | The table to save to.|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e53088a7-abf7-4005-b33a-14194d2db9b5"}}},{"cell_type":"markdown","source":["**Ejemplo**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0d91d85-397d-4741-87a3-d6afa8132744"}}},{"cell_type":"code","source":["%scala\n//Use JSON\n//El nombre me lo pone Databricks. La location es el directorio en el que se guarda el fichero. Si le quier camciar el nombre tengo que hacer un rename\nval location = \"\"\"/learning-spark-v2/sf-fire/Ejemplo_save_json\"\"\"\ndf.write.format(\"json\").mode(\"overwrite\").save(location)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efbbe703-0526-4675-8dcf-7e65db494e76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">location: String = /learning-spark-v2/sf-fire/Ejemplo_save_json\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">location: String = /learning-spark-v2/sf-fire/Ejemplo_save_json\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Fuentes de datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f6d43c8-1455-4bf9-96f8-49f887da5ba3"}}},{"cell_type":"markdown","source":["### Parquet\nLa fuente de datos predeterminada de Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e7f8fe6-6db6-4070-8f0e-6563246dc545"}}},{"cell_type":"markdown","source":["#### Leer de archivos Parquet en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cefa993-8033-4df3-9694-43f803c1ecc9"}}},{"cell_type":"code","source":["%scala\nval file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\"\"\"\nval df = spark.read.format(\"parquet\").load(file)\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"583f11cc-5516-4535-8715-8c083240cbaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"DEST_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"ORIGIN_COUNTRY_NAME","type":"string","nullable":true,"metadata":{}},{"name":"count","type":"long","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nfile: String = /databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\ndf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|    1|\n    United States|            Ireland|  264|\n    United States|              India|   69|\n            Egypt|      United States|   24|\nEquatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\nfile: String = /databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\ndf: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos Parquet en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0300b3d4-afb3-40f1-a43e-fb63d7af1840"}}},{"cell_type":"code","source":["%scala\n//Creamos la base de datos\nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING parquet\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/\" )\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c367b764-4bfb-4079-baae-a6e2defcc8a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"res19","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">res19: org.apache.spark.sql.DataFrame = []\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res19: org.apache.spark.sql.DataFrame = []\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n//Leemos la tabla\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33dcbd9a-785e-4940-90ee-e87d87cd3164"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|    1|\n    United States|            Ireland|  264|\n    United States|              India|   69|\n            Egypt|      United States|   24|\nEquatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos Parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b9c9de-226a-4850-bd88-387f85c370bf"}}},{"cell_type":"code","source":["%scala\ndf.write.format(\"parquet\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/parquet/df_parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c0c1bd7-7e48-4905-a94f-c79eb7e1663b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a tablas SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecca7c20-fcbf-4b41-9aeb-c44b4f0dc54a"}}},{"cell_type":"code","source":["%scala\ndf.write\n .mode(\"overwrite\")\n .saveAsTable(\"us_delay_flights_tbl\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd3c3613-3c5d-4b45-9a01-39dc7001fa05"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### JSON\nEn Spark se soporta tanto el formato sigle-line mode como el multiline mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55fee07a-ffc2-4fda-9bc7-1845f2aa0661"}}},{"cell_type":"markdown","source":["#### Leer de archivos JSON en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7430e7b-b699-4c8f-a541-29a5380a199b"}}},{"cell_type":"code","source":["%scala\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\nval df = spark.read.format(\"json\").load(file)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b811eaf4-9c27-4290-b1ce-d9070698665a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos JSON en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90d8687c-62be-4350-ad40-4b9519ab7ed1"}}},{"cell_type":"code","source":["%scala\n// Creamos la base de datos\nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING json\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n )\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"112ca504-de8f-490c-8524-8bbe6eeb40d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fccf204-ffa1-4dd8-884b-f696cd46c04a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["d\n#### Escribir DataFrames a archivos JSON"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e57d0a9c-93df-406e-b77e-bc3f96af8692"}}},{"cell_type":"code","source":["%scala\ndf.write.format(\"json\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/json/df_json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"687aa186-ebb2-4220-b1e4-3c7c8a558a4c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aca5f947-e3aa-4f56-943e-38c43adc3182"}}},{"cell_type":"markdown","source":["| Property name | Values | Meaning | Scope\n| --- | --- | --- | --- |\n| compression | none, uncompressed, bzip2, deflate, gzip, lz4, or snappy | Use this compression codec for writing. Note that read will only detect the compression or codec from the file extension.| Write |\n| dateFormat | yyyy-MM-dd or DateTimeFormatter | Use this format or any format from Java‚Äôs DateTime Formatter. | Read/ write |\n| multiLine | true, false | Use multiline mode. Default is false (single-line mode). | Read\n| allowUnquoted FieldNames | true, false | Allow unquoted JSON field names. Default is false. | Read"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38253d17-8a48-44c7-8d0a-191275eb731b"}}},{"cell_type":"markdown","source":["### CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e205437-566e-4f8b-89b8-52c19a2c8cd3"}}},{"cell_type":"markdown","source":["#### Leer de archivos CSV en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6941947b-1283-46e2-ab43-4731482f705e"}}},{"cell_type":"code","source":["%scala\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\nval schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\nval df = spark.read.format(\"csv\")\n .schema(schema)\n .option(\"header\", \"true\")\n .option(\"mode\", \"FAILFAST\") // Exit if any errors\n .option(\"nullValue\", \"\") // Replace any null data with quotes\n .load(file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fae9807-5eed-464e-8c8a-f262f6777992"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos CSV en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2594555f-8be2-44a0-b64d-5eb9866fa6da"}}},{"cell_type":"code","source":["%scala\n//Creamos la base de datos\nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING csv\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n header \"true\",\n inferSchema \"true\",\n mode \"FAILFAST\"\n )\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3fd61b3-0ae8-4182-a515-ea773154529a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dfa1df9-5b1c-4cc2-bcfa-1691985763e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos CSV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33a3d573-a308-4630-9a65-743317d02822"}}},{"cell_type":"code","source":["%scala\ndf.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8ea9d50-9fc3-45ab-9f39-4ad598fa6afe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**\n## TABLA\nP√°gina 128"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8906346-2c9c-4f96-a95e-007ba20cf377"}}},{"cell_type":"markdown","source":["### AVRO"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab375c0d-b806-423d-83d6-469a17e7973d"}}},{"cell_type":"markdown","source":["#### Leer de archivos AVRO en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a417864d-8ac9-4bee-b290-fe9b46f0ca2e"}}},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"avro\")\n.load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\")\ndf.show(false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cae5918-b7a4-4b7a-b260-e88e270cc839"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos AVRO en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5060d57-c5e3-47b2-8adb-6ef7e530d4f3"}}},{"cell_type":"code","source":["%scala\n//Creamos la base de datos\nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW episode_tbl\n USING avro\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n )\"\"\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"227ffa9a-b2f2-4c44-a30d-136ac906d978"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"SELECT * FROM episode_tbl\").show(false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1d98345-90e9-412f-9a31-5572201e9d2b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos AVRO"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ebc8c84-3bbc-4ea6-afd0-afb23dce9abe"}}},{"cell_type":"code","source":["%scala\ndf.write\n .format(\"avro\")\n .mode(\"overwrite\")\n .save(\"/tmp/data/avro/df_avro\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"967bc5d7-f0a1-4e42-bc0e-3890ed63d3a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Opciones**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccdc622e-de66-4d0e-bbfe-6b65605e6215"}}},{"cell_type":"markdown","source":["##TABLA\nP√°gina 130"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7db89dd-42b3-4a7d-b9c1-3673f383c899"}}},{"cell_type":"markdown","source":["### ORC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9526d959-3de3-44fe-8322-d485babe3d0a"}}},{"cell_type":"markdown","source":["#### Leer de archivos ORC en un DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c87076e-e115-4cef-a006-0ea92e73ec1f"}}},{"cell_type":"code","source":["%scala\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\nval df = spark.read.format(\"orc\").load(file)\ndf.show(10, false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0450d6e-dcf6-4c49-b0f1-c839fb198478"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Leer de archivos ORC en una tabla SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99a38a8a-39a8-4def-8dcb-fed6ec593722"}}},{"cell_type":"code","source":["%scala\n//Creo la base de datos \nspark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n USING orc\n OPTIONS (\n path \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n )\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dd7c090-543f-4351-9b23-feb67bf82d80"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38f9aa6e-ff13-4f18-ba5b-b10803b23e7b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Escribir DataFrames a archivos ORC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d37e4a31-12e8-477c-917d-33e66c8e742f"}}},{"cell_type":"code","source":["%scala\n// In Scala\ndf.write.format(\"orc\")\n .mode(\"overwrite\")\n .option(\"compression\", \"snappy\")\n .save(\"/tmp/data/orc/df_orc\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9086bac9-6b84-4cfe-b838-3646795e38f0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Im√°genes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33d67c97-cc22-4586-9388-46f54d58b47a"}}},{"cell_type":"markdown","source":["#### Reading an image file into a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eb44a01-0443-4268-9da6-d59d1f261a10"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.source.image\nval imageDir = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\nval imagesDF = spark.read.format(\"image\").load(imageDir)\nimagesDF.printSchema\nimagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\",\n \"label\").show(5, false)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b10a35d0-a966-4262-89da-ae68ad93c69f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Binary Files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1507e72-f68d-4886-b705-287e7c9a8140"}}},{"cell_type":"markdown","source":["#### Reading a binary file into a DataFrame\nEl siguiente c√≥digo lee todos los archivos JPG del directorio de entrada con cualquier directorio particionado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e10ce47c-6282-409a-9ac6-a4b4244ff399"}}},{"cell_type":"code","source":["%scala\nval path = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\nval binaryFilesDF = spark.read.format(\"binaryFile\")\n .option(\"pathGlobFilter\", \"*.jpg\")\n .load(path)\nbinaryFilesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15d8542d-f83e-45c9-81af-8ba9360dd383"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Para ignorar la detecci√≥n de datos de partici√≥n en un directorio, puede configurar ```recursiveFile Lookup to \"true\"```:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58f730f2-ef8d-4a9f-b042-d567286929d4"}}},{"cell_type":"code","source":["%scala\nval binaryFilesDF = spark.read.format(\"binaryFile\")\n .option(\"pathGlobFilter\", \"*.jpg\")\n .option(\"recursiveFileLookup\", \"true\")\n .load(path)\nbinaryFilesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56676f0e-015a-4c70-9805-edd7885cea2e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# APUNTES\nCon \\%fs puedo ejecutar comandos linux en el Notebook de Databricks. Se puede hacer un ls, ver directorios, crearlos y eliminarlos. En la siguiente web ```//https://docs.databricks.com/_static/notebooks/dbutils.html``` se observan todos los comandos disponibles.\n\n**Ejemplos**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d64f587-e1d9-42b7-9ae8-87f362e0d7df"}}},{"cell_type":"code","source":["%fs\nls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8ffe63-8aec-4001-b0a4-b05d7bff65a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/Ejemplo_save_json.json/","Ejemplo_save_json.json/",0,0],["dbfs:/FileStore/","FileStore/",0,0],["dbfs:/databricks-datasets/","databricks-datasets/",0,0],["dbfs:/databricks-results/","databricks-results/",0,0],["dbfs:/learning-spark-v2/","learning-spark-v2/",0,0],["dbfs:/tmp/","tmp/",0,0],["dbfs:/user/","user/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Ejemplo_save_json.json/</td><td>Ejemplo_save_json.json/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/</td><td>FileStore/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/</td><td>databricks-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-results/</td><td>databricks-results/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/tmp/</td><td>tmp/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/</td><td>user/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nls /learning-spark-v2/sf-fire/Ejemplo_save_json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12634765-b798-4d31-8d90-f46b55066d08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS","_SUCCESS",0,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276","_committed_2238006556770501276",114,1651142577000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276","_started_2238006556770501276",0,1651142576000],["dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json","part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json",21353,1651142576000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_committed_2238006556770501276</td><td>_committed_2238006556770501276</td><td>114</td><td>1651142577000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/_started_2238006556770501276</td><td>_started_2238006556770501276</td><td>0</td><td>1651142576000</td></tr><tr><td>dbfs:/learning-spark-v2/sf-fire/Ejemplo_save_json/part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>part-00000-tid-2238006556770501276-4704d525-889b-423b-8d58-a96e6976647e-280-1-c000.json</td><td>21353</td><td>1651142576000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nrm -r '/\"'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faebd6ad-92e5-4ec0-b4de-f5cb01e92e42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res38: Boolean = false\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res38: Boolean = false\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chapter 4_Scala","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2891140708077970}},"nbformat":4,"nbformat_minor":0}
